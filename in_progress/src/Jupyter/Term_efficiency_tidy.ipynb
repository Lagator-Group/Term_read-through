{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdKp-mmRcy2n"
   },
   "source": [
    "# Terminator efficiency\n",
    "\n",
    "Use Ju et al data to test how term efficiency depends on the promoter strength\n",
    "\n",
    "* Input: SeqEnd wig files and Excel file with terminator coords and props\n",
    "* Output: dataset of terminators with termination efficiencies, read-through rates; correlation graphs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3SMlvYhMPuy"
   },
   "source": [
    "# Prepare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjldMM_cPYqS"
   },
   "source": [
    "## Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:15.521079Z",
     "start_time": "2023-05-04T10:06:15.389321Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20322,
     "status": "ok",
     "timestamp": 1660661253142,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "9fQbocSnK-hb",
    "outputId": "6a8c8b7a-57cb-4a8a-b5a1-3b223a3a0b03"
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl --upgrade --pre\n",
    "!pip3 install gffpandas\n",
    "!python -m pip install -U mpltern\n",
    "!pip install pingouin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbYBqpLkMxGr"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:17.822136Z",
     "start_time": "2023-05-04T10:06:15.523314Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1723,
     "status": "ok",
     "timestamp": 1660661254857,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "ZbF9too5M1rZ",
    "outputId": "899372b0-4734-430f-a440-41c04d6977ea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import copy\n",
    "from sys import path as syspath\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import pathlib\n",
    "from random import randint, seed\n",
    "import gzip\n",
    "import gffpandas.gffpandas as gffpd\n",
    "import mpltern\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjVIRUtsNFMm"
   },
   "source": [
    "## Select dataset for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_set = 'new_primary_transcipts'\n",
    "\n",
    "term_mechanism = 'Rho'#'intrinsic'#\n",
    "\n",
    "if term_mechanism == 'Rho':\n",
    "    term_mech_prefix = 'Rho'\n",
    "elif term_mechanism == 'intrinsic':\n",
    "    term_mech_prefix = 'IT'\n",
    "    \n",
    "is_bidir = False # Are we looking at bidirectional only (True) or unidirctional promoters only (False)\n",
    "if is_bidir:\n",
    "    bidir_prefix = 'BI'\n",
    "else:\n",
    "    bidir_prefix = 'UNI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:17.854880Z",
     "start_time": "2023-05-04T10:06:17.825868Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1660661278598,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "iAiiMafO8Lwe",
    "outputId": "6b7ebe68-13bf-4119-dd1c-9cad6ae72efa"
   },
   "outputs": [],
   "source": [
    "# Working dir\n",
    "cwd = pathlib.Path(os.getcwd())\n",
    "print(cwd)\n",
    "\n",
    "# Input folder\n",
    "input_folderpath = cwd.parents[2] / 'input/data/2023-04_18_from_Ju_et_al/GSE117737_RAW'\n",
    "print(input_folderpath)\n",
    "assert input_folderpath.is_dir()\n",
    "\n",
    "# Input terminators excel file\n",
    "input_xls_terms_filename = input_folderpath.parents[0] / '41564_2019_500_MOESM4_ESM.xlsx'\n",
    "assert input_xls_terms_filename.is_file()\n",
    "\n",
    "# Input BIDIRECTIONAL terminators excel file\n",
    "input_xls_bidir_terms_filename = input_folderpath.parents[0] / '41564_2019_500_MOESM6_ESM.xlsx'\n",
    "assert input_xls_bidir_terms_filename.is_file()\n",
    "\n",
    "# Output files\n",
    "plots_out_folder = cwd.parents[2] / 'output/plots'\n",
    "final_excel_table = str(cwd.parents[2]) + '/output/output_data/'+bidir_prefix+'_'+term_mechanism+'_final_df.xlsx'\n",
    "print(final_excel_table)\n",
    "Ju_full_excel_table = cwd.parents[2] / 'output/output_data/Ju_full_data.xlsx'\n",
    "all_Ju_terms_fasta_file_path = cwd.parents[2] / 'in_progress/processed_data/all_Ju_terms.fasta'\n",
    "\n",
    "# Experiments\n",
    "    \n",
    "if experiment_set == 'new_primary_transcipts':\n",
    "    processed_data_folderpath = cwd.parents[2] / 'in_progress/processed_data/2023-12-01_Primary_transcripts'\n",
    "    phases = ['log', 'stat']\n",
    "    strands = ['plus', 'minus']\n",
    "    log_reps = ['rep_1','rep_2','rep_3','rep_4','rep_5']\n",
    "    stat_reps = ['rep_1','rep_2','rep_3']\n",
    "\n",
    "    sign_strand = {}\n",
    "    sign_strand['plus'] = '+'\n",
    "    sign_strand['minus'] = '-'\n",
    "\n",
    "    # Input SeqEnd wig files\n",
    "    input_filepaths = {}\n",
    "\n",
    "    ## Log and stationary phases\n",
    "    input_filepaths['log'] = {}\n",
    "    input_filepaths['stat'] = {}\n",
    "\n",
    "    ## Plus & minus strands\n",
    "    input_filepaths['log']['plus'] = {}\n",
    "    input_filepaths['log']['minus'] = {}\n",
    "\n",
    "    input_filepaths['stat']['plus'] = {}\n",
    "    input_filepaths['stat']['minus'] = {}\n",
    "\n",
    "    ## Replicates\n",
    "\n",
    "    ### Log phase\n",
    "    #### Plus strand\n",
    "    input_filepaths['log']['plus']['rep_1'] = processed_data_folderpath / 'Log_phase/Log_rep1_plus.wig.gz'\n",
    "    assert input_filepaths['log']['plus']['rep_1'].is_file()\n",
    "\n",
    "    input_filepaths['log']['plus']['rep_2'] = processed_data_folderpath / 'Log_phase/Log_rep2_plus.wig.gz'\n",
    "    assert input_filepaths['log']['plus']['rep_2'].is_file()\n",
    "    \n",
    "    input_filepaths['log']['plus']['rep_3'] = processed_data_folderpath / 'Log_phase/Log_rep3_plus.wig.gz'\n",
    "    assert input_filepaths['log']['plus']['rep_3'].is_file()\n",
    "    \n",
    "    input_filepaths['log']['plus']['rep_4'] = processed_data_folderpath / 'Log_phase/Log_rep4_plus.wig.gz'\n",
    "    assert input_filepaths['log']['plus']['rep_4'].is_file()\n",
    "    \n",
    "    input_filepaths['log']['plus']['rep_5'] = processed_data_folderpath / 'Log_phase/Log_rep5_plus.wig.gz'\n",
    "    assert input_filepaths['log']['plus']['rep_5'].is_file()\n",
    "    \n",
    "    #### Minus strand\n",
    "    input_filepaths['log']['minus']['rep_1'] = processed_data_folderpath / 'Log_phase/Log_rep1_minus.wig.gz'\n",
    "    assert input_filepaths['log']['minus']['rep_1'].is_file()\n",
    "\n",
    "    input_filepaths['log']['minus']['rep_2'] = processed_data_folderpath / 'Log_phase/Log_rep2_minus.wig.gz'\n",
    "    assert input_filepaths['log']['minus']['rep_2'].is_file()\n",
    "    \n",
    "    input_filepaths['log']['minus']['rep_3'] = processed_data_folderpath / 'Log_phase/Log_rep3_minus.wig.gz'\n",
    "    assert input_filepaths['log']['minus']['rep_3'].is_file()\n",
    "    \n",
    "    input_filepaths['log']['minus']['rep_4'] = processed_data_folderpath / 'Log_phase/Log_rep4_minus.wig.gz'\n",
    "    assert input_filepaths['log']['minus']['rep_4'].is_file()\n",
    "    \n",
    "    input_filepaths['log']['minus']['rep_5'] = processed_data_folderpath / 'Log_phase/Log_rep5_minus.wig.gz'\n",
    "    assert input_filepaths['log']['minus']['rep_5'].is_file()\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    ### Stationary phase\n",
    "    #### Plus strand\n",
    "    input_filepaths['stat']['plus']['rep_1'] = processed_data_folderpath / 'Stat_phase/Stat_rep1_plus.wig.gz'\n",
    "    assert input_filepaths['stat']['plus']['rep_1'].is_file()\n",
    "    \n",
    "    input_filepaths['stat']['plus']['rep_2'] = processed_data_folderpath / 'Stat_phase/Stat_rep2_plus.wig.gz'\n",
    "    assert input_filepaths['stat']['plus']['rep_2'].is_file()\n",
    "    \n",
    "    input_filepaths['stat']['plus']['rep_3'] = processed_data_folderpath / 'Stat_phase/Stat_rep3_plus.wig.gz'\n",
    "    assert input_filepaths['stat']['plus']['rep_3'].is_file()\n",
    "\n",
    "    #### Minus strand\n",
    "    input_filepaths['stat']['minus']['rep_1'] = processed_data_folderpath / 'Stat_phase/Stat_rep1_minus.wig.gz'\n",
    "    assert input_filepaths['stat']['minus']['rep_1'].is_file()\n",
    "    \n",
    "    input_filepaths['stat']['minus']['rep_2'] = processed_data_folderpath / 'Stat_phase/Stat_rep2_minus.wig.gz'\n",
    "    assert input_filepaths['stat']['minus']['rep_2'].is_file()\n",
    "    \n",
    "    input_filepaths['stat']['minus']['rep_3'] = processed_data_folderpath / 'Stat_phase/Stat_rep3_minus.wig.gz'\n",
    "    assert input_filepaths['stat']['minus']['rep_3'].is_file()\n",
    "    \n",
    "    \n",
    "    #=======================================================================================================\n",
    "    \n",
    "    # Input SeqEnd GFF files\n",
    "    GFF_input_filepaths = {}\n",
    "\n",
    "    ## Log and stationary phases\n",
    "    GFF_input_filepaths['log'] = {}\n",
    "    GFF_input_filepaths['stat'] = {}\n",
    "    \n",
    "    ## Replicates\n",
    "\n",
    "    ### Log phase\n",
    "    \n",
    "    GFF_input_filepaths['log']['rep_1'] = processed_data_folderpath / 'Log_phase/Log_rep1.gff'\n",
    "    assert GFF_input_filepaths['log']['rep_1'].is_file()\n",
    "\n",
    "    GFF_input_filepaths['log']['rep_2'] = processed_data_folderpath / 'Log_phase/Log_rep2.gff'\n",
    "    assert GFF_input_filepaths['log']['rep_2'].is_file()\n",
    "    \n",
    "    GFF_input_filepaths['log']['rep_3'] = processed_data_folderpath / 'Log_phase/Log_rep3.gff'\n",
    "    assert GFF_input_filepaths['log']['rep_3'].is_file()\n",
    "    \n",
    "    GFF_input_filepaths['log']['rep_4'] = processed_data_folderpath / 'Log_phase/Log_rep4.gff'\n",
    "    assert GFF_input_filepaths['log']['rep_4'].is_file()\n",
    "    \n",
    "    GFF_input_filepaths['log']['rep_5'] = processed_data_folderpath / 'Log_phase/Log_rep5.gff'\n",
    "    assert GFF_input_filepaths['log']['rep_5'].is_file()\n",
    "\n",
    "    ### Stationary phase\n",
    "    \n",
    "    GFF_input_filepaths['stat']['rep_1'] = processed_data_folderpath / 'Stat_phase/Stat_rep1.gff'\n",
    "    assert GFF_input_filepaths['stat']['rep_1'].is_file()\n",
    "    \n",
    "    GFF_input_filepaths['stat']['rep_2'] = processed_data_folderpath / 'Stat_phase/Stat_rep2.gff'\n",
    "    assert GFF_input_filepaths['stat']['rep_2'].is_file()\n",
    "    \n",
    "    GFF_input_filepaths['stat']['rep_3'] = processed_data_folderpath / 'Stat_phase/Stat_rep3.gff'\n",
    "    assert GFF_input_filepaths['stat']['rep_3'].is_file()\n",
    "    \n",
    "    \n",
    "    # Input suppl table experimental terms\n",
    "    \n",
    "    exp_terms_filename = input_folderpath.parents[1] / '2024-02_13_from_BMC_Biol_suppl/Suppl_data_1.xlsx'\n",
    "    assert exp_terms_filename.is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilTNEkDJlkIq"
   },
   "source": [
    "## Set constants & parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPG20utaloGC"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:17.880177Z",
     "start_time": "2023-05-04T10:06:17.860361Z"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1660661282118,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "94_-d_ntlwym"
   },
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed(3)\n",
    "\n",
    "# Nucleotides\n",
    "bases = \"acgt\"\n",
    "lett_to_index = dict(zip(bases, range(4)))\n",
    "\n",
    "# Display options (adjusted to Mac)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Normalise to read coverage?\n",
    "is_expr_norm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKKqYNStl7z3"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:17.884811Z",
     "start_time": "2023-05-04T10:06:17.882311Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660661282119,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "60r7TIhkmEgn"
   },
   "outputs": [],
   "source": [
    "# Normalisation factor while calculating read-through rate: \n",
    "# t+rt - terminated & read-through trancripts count, \n",
    "# t+rt+ri - terminated & read-through & (re)initiated trancripts count \n",
    "norm_factor = 't+rt' # 't+rt+ri'\n",
    "\n",
    "# Whether or not to delete efficiency = 1, 0, <0\n",
    "is_eff_1_deleted = True\n",
    "is_eff_0_deleted = True\n",
    "is_neg_eff_deleted = True\n",
    "\n",
    "# Efficiency tresholds\n",
    "max_eff_th = 1000\n",
    "\n",
    "# Expression tresholds\n",
    "min_expr_th = 0.1\n",
    "max_expr_th = 15\n",
    "\n",
    "# Length of fragment before and after terminator\n",
    "\n",
    "before_term_len = 130\n",
    "before_exclude_len = 60\n",
    "after_gap = 10 # space to ignore after term coord\n",
    "after_term_len = 80\n",
    "\n",
    "before_region_suffix = '_before_reg('+str(-before_term_len)+','+str(-before_exclude_len)+')'\n",
    "\n",
    "# Efficiency (wig files) or readthrough/reinitiation/termination (gff files)\n",
    "eff_measure = 'readthrough'#'reinitiation'#'termination'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow_ttVmWevp0"
   },
   "source": [
    "## Install and import in-house functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:17.904307Z",
     "start_time": "2023-05-04T10:06:17.887429Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_between_full(df_value, df_interval, value_col, interval_cols, closed, data_type):\n",
    "    # Inner join by interval (SQL between analog)\n",
    "    \n",
    "    ## df_interval[interval_cols[0]] <(=) df_value[value] <(=) df_interval[interval_cols[1]]\n",
    "    ## closed: both - [a,b]; right - (a,b]; left - [a,b); neither - (a,b)\n",
    "    ## data_type: int64 for int or smth else for float\n",
    "    \n",
    "    # Drop NAs and inf\n",
    "    df_interval[interval_cols].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_interval = df_interval.dropna(subset=interval_cols)\n",
    "    df_value.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_value = df_value.dropna(subset=[value_col])\n",
    "    \n",
    "    # Unify data types\n",
    "    df_interval.loc[:,interval_cols] = df_interval.loc[:,interval_cols].astype(data_type)\n",
    "    df_value.loc[:,value_col] = df_value.loc[:,value_col].astype(data_type)\n",
    "\n",
    "    \n",
    "    a = df_value[value_col].values\n",
    "    bl = df_interval[interval_cols[0]].values\n",
    "    bh = df_interval[interval_cols[1]].values\n",
    "\n",
    "    if closed == 'both': # [bl,bh]\n",
    "        i, j = np.where((a[:, None] >= bl) & (a[:, None] <= bh))\n",
    "    elif closed == 'right': # (bl,bh]\n",
    "        i, j = np.where((a[:, None] > bl) & (a[:, None] <= bh))\n",
    "    elif closed == 'left': # [bl,bh)\n",
    "        i, j = np.where((a[:, None] >= bl) & (a[:, None] < bh))\n",
    "    elif closed == 'neither': # (bl,bh)\n",
    "        i, j = np.where((a[:, None] > bl) & (a[:, None] < bh))\n",
    "    else:\n",
    "        raise TypeError('Uncknown type of interaval: '+str(closed))\n",
    "    \n",
    "\n",
    "    merged_df = pd.DataFrame(\n",
    "        np.column_stack([df_value.values[i], df_interval.values[j]]),\n",
    "        columns=df_value.columns.append(df_interval.columns)\n",
    "    )\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Count trancripts +- 10 nt around TTS from wig files\n",
    "def before_after_counts(coords, file_to_open, strand, before_term_len, after_term_len, before_exclude_len): # to change\n",
    "    # Coord of TTS is defined as the last base of the transcript\n",
    "    # after_gap -  intermidiate nts around TSS were excluded from counting\n",
    "    coords.sort()\n",
    "    coords_len = len(coords)\n",
    "    before_cnts = []\n",
    "    after_cnts = []\n",
    "    i = 0\n",
    "\n",
    "    with gzip.open(file_to_open,'r') as fin:\n",
    "        if strand == 'plus':\n",
    "            before_cnt = 0\n",
    "            after_cnt = 0\n",
    "            curr_line_num = -1\n",
    "            #print('i = '+ str(i)+', coord = '+ str(coords[i]))\n",
    "            for line in fin:\n",
    "                curr_coord = coords[i]\n",
    "                #print(str(curr_line_num)+ ' '+ str(line.strip()))\n",
    "                if (curr_line_num >= curr_coord - before_term_len) and (curr_line_num < curr_coord - before_exclude_len): \n",
    "                    before_cnt += int(line.strip())\n",
    "                elif (curr_line_num > curr_coord + after_gap) and (curr_line_num <= curr_coord + after_term_len + after_gap): # exclude 2 intermidiate nt\n",
    "                    after_cnt += int(line.strip())\n",
    "                    if curr_line_num == curr_coord + after_term_len + after_gap:\n",
    "                        before_cnts.append(before_cnt/before_term_len)\n",
    "                        after_cnts.append(after_cnt/after_term_len)\n",
    "                        i += 1\n",
    "                        if i < coords_len: \n",
    "                            #print('i = '+ str(i)+', coord = '+ str(coords[i]))\n",
    "                            before_cnt = 0\n",
    "                            after_cnt = 0\n",
    "                        else:\n",
    "                            before_cnt = 0\n",
    "                            after_cnt = 0\n",
    "                            i = 0\n",
    "                            break\n",
    "                            \n",
    "                curr_line_num += 1\n",
    "                \n",
    "        elif strand == 'minus':\n",
    "            before_cnt = 0\n",
    "            after_cnt = 0\n",
    "            curr_line_num = -1\n",
    "            #print('i = '+ str(i)+', coord = '+ str(coords[i]))\n",
    "            for line in fin:\n",
    "                curr_coord = coords[i]\n",
    "                #print(str(curr_line_num)+ ' '+ str(line.strip))\n",
    "                if (curr_line_num >= curr_coord - after_term_len - after_gap) and (curr_line_num < curr_coord - after_gap): # exclude 3 intermidiate nt\n",
    "                    after_cnt += int(line.strip())\n",
    "                elif (curr_line_num > curr_coord + before_exclude_len) and (curr_line_num <= curr_coord + before_term_len): # exclude 3 intermidiate nt\n",
    "                    before_cnt += int(line.strip())\n",
    "                    if curr_line_num == curr_coord + before_term_len - 1:\n",
    "                        before_cnts.append(before_cnt/before_term_len)\n",
    "                        after_cnts.append(after_cnt/after_term_len)\n",
    "                        i += 1\n",
    "                        if i < coords_len: \n",
    "                            #print('i = '+ str(i)+', coord = '+ str(coords[i]))\n",
    "                            before_cnt = 0\n",
    "                            after_cnt = 0\n",
    "                        else:\n",
    "                            before_cnt = 0\n",
    "                            after_cnt = 0\n",
    "                            i = 0\n",
    "                            break\n",
    "                            \n",
    "                curr_line_num += 1\n",
    "    \n",
    "    return coords, before_cnts, after_cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtcarInMmoJx"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK7wA4CCmqfu"
   },
   "source": [
    "Import terminator data, filter & modify the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "belJvEs1niPV"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wQfmCOi_Acc"
   },
   "source": [
    "### Import terminator files (with coords and props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:18.439202Z",
     "start_time": "2023-05-04T10:06:17.918110Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 4851,
     "status": "ok",
     "timestamp": 1660661286964,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "sCfKsuLI-8ps",
    "outputId": "0789deb1-2301-4bb7-e113-c3a56b787032"
   },
   "outputs": [],
   "source": [
    "# Import terminators dataframe\n",
    "in_terms_df = pd.read_excel(input_xls_terms_filename, sheet_name='Sheet1',header=1, names=None, index_col=0, dtype=None, skiprows=0)\n",
    "## Rename some columns\n",
    "in_terms_df = in_terms_df.rename(columns={\" ∆G (kcal/mol)\": \"dG\", \"5'_flank_sequence\": \"5_prime_seq\", \"Number_of_A_around_5'flank\":\"Number_of_A_around_5_prime\", \"3'_flank_sequence\":\"3_prime_seq\", \"Number_of_U_around_5'flank\":\"Number_of_U_around_5_prime\", \"%GC_of_stem_region(left)\":\"GC_content_of_stem_region(left)\" })\n",
    "\n",
    "print('in_terms_df:')\n",
    "display(in_terms_df)\n",
    "\n",
    "# Import bidir terminators dataframe\n",
    "in_bidir_terms_df = pd.read_excel(input_xls_bidir_terms_filename, sheet_name='Extended Data Table 5',header=1, names=None, index_col=0, dtype=None, skiprows=0)\n",
    "## Rename some columns\n",
    "in_bidir_terms_df = in_bidir_terms_df.rename(columns={\"Overlap_start_site(5'_of _plus_strand)\": \"Overlap_start_site\", \"Overlap_end_site(5'_of _plus_strand)\":\"Overlap_end_site\", \" ∆G (kcal/mol)\":\"dG\", \"5'_flank_of_hairpin_structure_around_overlapping_region\":\"5_prime_of_hairpin_structure_around_overlapping_region\", \"Number_of_nucleotide_A_of_5'_flank\":\"Number_of_nucleotide_A_of_5_prime\", \"3'_flank_of_hairpin_structure_around_overlapping_region\":\"3_prime_of_hairpin_structure_around_overlapping_region\", \"Number_of_nucleotide_U_of_3'flank\":'Number_of_nucleotide_U_of_3_prime', \"GC%_of_stem_RNA_sequence\":\"GC_content_of_stem_RNA_sequence\"})\n",
    "\n",
    "print('in_bidir_terms_df:')\n",
    "display(in_bidir_terms_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1CBqZvdn4I7"
   },
   "source": [
    "## Filter and modily data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column is_bidir to terms_df, fill NAs, export full dataset to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:18.529311Z",
     "start_time": "2023-05-04T10:06:18.449171Z"
    },
    "executionInfo": {
     "elapsed": 157316,
     "status": "ok",
     "timestamp": 1660661444277,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "CkcqROdLoVuJ"
   },
   "outputs": [],
   "source": [
    "# Interval-based join between bidir_terms_df and terms_df\n",
    "## Add is_bidir to bidir_terms_df\n",
    "in_bidir_terms_df['is_bidir'] = True\n",
    "## Join\n",
    "bidir_terms_df = merge_between_full(in_terms_df, in_bidir_terms_df, \"TTS_position\", [\"Overlap_start_site\",\"Overlap_end_site\"], \"both\", \"int64\")\n",
    "\n",
    "# Left join between in_terms df and bidir_terms_df to include is_bidir colums\n",
    "## Make cute version of bidir_terms_df\n",
    "cute_bidir_terms_df = bidir_terms_df[['TTS_position', 'TTS_strand','is_bidir']]\n",
    "# Left join\n",
    "terms_df = pd.merge(in_terms_df, cute_bidir_terms_df, how='left', on=['TTS_position','TTS_strand'])\n",
    "## Delete clutter and fill NA\n",
    "terms_df['is_bidir'].replace([np.nan], False, inplace=True)\n",
    "terms_df['intrinsic_TTS'].replace([np.nan], False, inplace=True)\n",
    "terms_df['intrinsic_TTS'].replace('intrinsic_TTS', True, inplace=True)\n",
    "terms_df['TTS_type_rho_dependent'].replace([np.nan], False, inplace=True)\n",
    "terms_df['TTS_type_rho_dependent'].replace('rho-dependent', True, inplace=True)\n",
    "\n",
    "# Add unique TTS_id\n",
    "terms_df['TTS_id'] = terms_df['TTS_position'].astype(str) +'_'+ terms_df['TTS_strand'].astype(str)\n",
    "## Make TTS_id the first column\n",
    "terms_df.insert(0, 'TTS_id', terms_df.pop('TTS_id'))\n",
    "print('terms_df:')\n",
    "display(terms_df)\n",
    "\n",
    "# Export full Ju et all dataset to Excel\n",
    "terms_df.to_excel(Ju_full_excel_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only intrinsic (xor Rho-dependent)  unidirectional (xor bidirectional) terminators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:06:18.573008Z",
     "start_time": "2023-05-04T10:06:18.531478Z"
    }
   },
   "outputs": [],
   "source": [
    "if term_mechanism == 'Rho':\n",
    "    if is_bidir:\n",
    "        final_terms_df = terms_df[~terms_df['intrinsic_TTS'] & terms_df['is_bidir'] & terms_df['TTS_type_rho_dependent']]\n",
    "    else:\n",
    "        final_terms_df = terms_df[~terms_df['intrinsic_TTS'] & ~terms_df['is_bidir'] & terms_df['TTS_type_rho_dependent']]\n",
    "elif term_mechanism == 'intrinsic':\n",
    "    if is_bidir:\n",
    "        final_terms_df = terms_df[terms_df['intrinsic_TTS'] & (terms_df['is_bidir']) & ~(terms_df['TTS_type_rho_dependent'])]\n",
    "    else:\n",
    "        final_terms_df = terms_df[terms_df['intrinsic_TTS'] & ~(terms_df['is_bidir']) & ~(terms_df['TTS_type_rho_dependent'])]\n",
    "\n",
    "display(final_terms_df)\n",
    "\n",
    "cute_final_terms_df = final_terms_df[['TTS_position','TTS_strand']]\n",
    "display(cute_final_terms_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b4YLEZ8ozan"
   },
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term efficiency (wig files-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QEtSy4hpRmy"
   },
   "source": [
    "### Loop over wig files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T12:20:59.237053Z",
     "start_time": "2022-10-13T12:20:59.231157Z"
    }
   },
   "source": [
    "Loop over all input files, open each file, run counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:07:01.947554Z",
     "start_time": "2023-05-04T10:06:18.576335Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1660661445311,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "uJzKyAXyozC_",
    "outputId": "bf4e0469-f1a7-4472-ef5c-e747e9f4a368"
   },
   "outputs": [],
   "source": [
    "for phase in phases:\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "     \n",
    "    for strand in strands:\n",
    "        curr_terms_df = cute_final_terms_df[cute_final_terms_df['TTS_strand']==sign_strand[strand]]\n",
    "        effs = []\n",
    "        before_exprs = []\n",
    "        for rep in reps:\n",
    "            file_to_open = input_filepaths[phase][strand][rep]\n",
    "            print('the file opened: '+ str(file_to_open))\n",
    "            curr_colname_suffix = phase+'_'+rep\n",
    "            \n",
    "            ## Run counting script\n",
    "            coords = curr_terms_df['TTS_position'].tolist()\n",
    "            coords, before_cnts, after_cnts = before_after_counts(coords, file_to_open, strand, before_term_len, after_term_len, before_exclude_len)\n",
    "            eff = []\n",
    "            for i, j in zip(after_cnts, before_cnts):\n",
    "                if j != 0:\n",
    "                    eff.append((j - i) / j)\n",
    "                else:\n",
    "                    eff.append(np.nan)\n",
    "\n",
    "            effs.append(eff)\n",
    "            \n",
    "            curr_cnt_terms_df = pd.DataFrame(list(zip(coords, before_cnts, after_cnts, eff)), columns =['TTS_position', curr_colname_suffix+'_before_cnt', curr_colname_suffix+'_after_cnt', curr_colname_suffix+'_eff'])\n",
    "            curr_cnt_terms_df['TTS_strand'] = sign_strand[strand]\n",
    "            \n",
    "  \n",
    "            \n",
    "            ### Inner join curr_cnt_terms_df to curr_terms_df for each strand separately\n",
    "            curr_terms_df = pd.merge(curr_terms_df, curr_cnt_terms_df, how='inner', on=['TTS_position','TTS_strand'])\n",
    "        \n",
    "        if strand == 'plus':\n",
    "            plus_curr_terms_df = copy.deepcopy(curr_terms_df)\n",
    "        elif strand == 'minus':\n",
    "            minus_curr_terms_df = copy.deepcopy(curr_terms_df)\n",
    "    \n",
    "    plus_minus_curr_term_df = pd.concat([plus_curr_terms_df, minus_curr_terms_df])\n",
    "    \n",
    "    # Left join counts to final_terms_df\n",
    "    final_terms_df = pd.merge(final_terms_df, plus_minus_curr_term_df, how='left', on=['TTS_position','TTS_strand'])\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise before_counts to total coverage\n",
    "* Normilise each repicate\n",
    "* Add avareges over normalised counts across replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normilise each repicate\n",
    "for phase in phases:\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "    \n",
    "    for rep in reps:\n",
    "        curr_colname_suffix = phase+'_'+rep\n",
    "        curr_before_cnt_colname = curr_colname_suffix+'_before_cnt'\n",
    "        curr_after_cnt_colname = curr_colname_suffix+'_after_cnt'\n",
    "        norm_expr_colname = curr_colname_suffix+'_norm_expr'\n",
    "        #print(curr_before_cnt_colname)\n",
    "        \n",
    "        ## Normalise expressions\n",
    "        curr_before_cnts = final_terms_df[curr_before_cnt_colname].tolist()\n",
    "        avg_expr = sum(curr_before_cnts)/len(curr_before_cnts)\n",
    "        #print(avg_expr)\n",
    "        norm_exprs = [val/avg_expr for val in curr_before_cnts]\n",
    "       \n",
    "        ## Insert into databse after curr_after_cnt_colname\n",
    "        loc  = final_terms_df.columns.get_loc(curr_after_cnt_colname) + 1\n",
    "        final_terms_df.insert(loc, norm_expr_colname, norm_exprs)\n",
    "\n",
    "# Add avareges over normalised counts across replicates\n",
    "for phase in phases:\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "    \n",
    "    # Collect norm expr & eff colnames to average\n",
    "    norm_expr_colnames = []\n",
    "    eff_colnames = []\n",
    "    for rep in reps:\n",
    "        curr_colname_suffix = phase+'_'+rep\n",
    "        \n",
    "        curr_norm_expr_colname = curr_colname_suffix+'_norm_expr'\n",
    "        norm_expr_colnames.append(curr_norm_expr_colname)\n",
    "        \n",
    "        curr_eff_colname = curr_colname_suffix+'_eff'\n",
    "        eff_colnames.append(curr_eff_colname)\n",
    "    \n",
    "    phase_avg_expr_colname = phase+'_avg_norm_expr'\n",
    "    final_terms_df[phase_avg_expr_colname] = final_terms_df[norm_expr_colnames].mean(axis='columns')\n",
    "    \n",
    "    phase_avg_eff_colname = phase+'_avg_eff'\n",
    "    final_terms_df[phase_avg_eff_colname] = final_terms_df[eff_colnames].mean(axis='columns')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readthrough vs reinitiation (GFF trancripts-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count terminated, read-through, reinitiated trancripts (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_term_cnt(transcript_df, term_coord, strand, norm_factor):\n",
    "# Count 3 numbers for each terminator: terminated trancripts, read-through transcripts, reinitiated trancripts \n",
    "# norm_factor: what to normilize to when calculating rt, ri percentages: 't+rt+ri' or 't+rt'\n",
    "\n",
    "# NOTE:\n",
    "# Terminator windows start/stop are in line with transcription direction (before_start, before_end, after_start, after_end)\n",
    "# Transcript start/stop are in line with reference genome direction (transcripts.df['start'], transcripts.df['end'])\n",
    "\n",
    "    if strand == '+':\n",
    "        before_start = term_coord - before_term_len\n",
    "        before_end = term_coord - before_exclude_len\n",
    "        #print('before_start = '+str(before_start))\n",
    "        #print('before_end = '+str(before_end))\n",
    "\n",
    "        #print('Before window overlap:')\n",
    "        before_overlap = transcript_df[(transcript_df['start']<before_end) & (transcript_df['end']>=before_start) & (transcript_df['strand']==strand)]\n",
    "        #display(before_overlap)\n",
    "\n",
    "        after_start = term_coord + after_gap\n",
    "        after_end = term_coord + after_term_len + after_gap\n",
    "        #print('after_start = '+str(after_start))\n",
    "        #print('after_end = '+str(after_end))\n",
    "\n",
    "        #print('After window overlap:')\n",
    "        after_overlap = transcript_df[(transcript_df['start']<=after_end) & (transcript_df['end']>after_start) & (transcript_df['strand']==strand)]\n",
    "        #display(after_overlap)\n",
    "\n",
    "    elif strand == '-':\n",
    "        before_start = term_coord+before_term_len\n",
    "        before_end = term_coord+before_exclude_len\n",
    "        #print('before_start = '+str(before_start))\n",
    "        #print('before_end = '+str(before_end))\n",
    "\n",
    "        #print('Before window overlap:')\n",
    "        before_overlap = transcript_df[(transcript_df['start']<=before_start) & (transcript_df['end']>before_end) & (transcript_df['strand']==strand)]\n",
    "        #display(before_overlap)\n",
    "\n",
    "        after_start = term_coord - after_gap\n",
    "        after_end = term_coord - after_term_len - after_gap\n",
    "        #print('after_start = '+str(after_start))\n",
    "        #print('after_end = '+str(after_end))\n",
    "\n",
    "        #print('After window overlap:')\n",
    "        after_overlap = transcript_df[(transcript_df['start']<after_start) & (transcript_df['end']>=after_end) & (transcript_df['strand']==strand)]\n",
    "        #display(after_overlap)\n",
    "\n",
    "    else:\n",
    "        raise TypeError('What a strand is it?')\n",
    "\n",
    "    before_tr_ids = set(before_overlap['transcript_id'].tolist())\n",
    "    #print(before_tr_ids)\n",
    "\n",
    "    after_tr_ids = set(after_overlap['transcript_id'].tolist())\n",
    "    #print(after_tr_ids)\n",
    "\n",
    "    # Terminated\n",
    "    Terminated = before_tr_ids.difference(after_tr_ids)\n",
    "    Terminated_cnt = len(Terminated)\n",
    "    #print('Terminated:')\n",
    "    #print(Terminated_cnt)\n",
    "\n",
    "    # Read-through\n",
    "    Readthrough = before_tr_ids.intersection(after_tr_ids)\n",
    "    Readthrough_cnt = len(Readthrough)\n",
    "    #print('Readthrough:')\n",
    "    #print(Readthrough_cnt)\n",
    "\n",
    "    # Reinitiated\n",
    "    Reinitiated = after_tr_ids.difference(before_tr_ids)\n",
    "    Reinitiated_cnt = len(Reinitiated)\n",
    "    #print('Reinitiated:')\n",
    "    #print(Reinitiated_cnt)\n",
    "    \n",
    "    # Percentages\n",
    "    if norm_factor == 't+rt+ri':\n",
    "        \n",
    "        All_cnt = Terminated_cnt + Readthrough_cnt + Reinitiated_cnt\n",
    "        if All_cnt == 0:\n",
    "            Terminated_pc = np.nan\n",
    "            Readthrough_pc = np.nan\n",
    "            Reinitiated_pc = np.nan\n",
    "\n",
    "        else:\n",
    "            Terminated_pc = Terminated_cnt / All_cnt\n",
    "            Readthrough_pc = Readthrough_cnt / All_cnt\n",
    "            Reinitiated_pc = 1 - Terminated_pc - Readthrough_pc\n",
    "            \n",
    "    elif norm_factor == 't+rt':\n",
    "        \n",
    "        All_cnt = Terminated_cnt + Readthrough_cnt\n",
    "        if All_cnt == 0:\n",
    "            Terminated_pc = np.nan\n",
    "            Readthrough_pc = np.nan\n",
    "            Reinitiated_pc = np.nan\n",
    "\n",
    "        else:\n",
    "            Terminated_pc = Terminated_cnt / All_cnt\n",
    "            Readthrough_pc = Readthrough_cnt / All_cnt\n",
    "            Reinitiated_pc = Reinitiated_cnt / All_cnt\n",
    "    else:\n",
    "        raise TypeError('Unknown norm_factor: '+ str(norm_factor))\n",
    "\n",
    "            \n",
    "    \n",
    "    return Terminated_cnt, Readthrough_cnt, Reinitiated_cnt, Terminated_pc, Readthrough_pc, Reinitiated_pc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count t, rt, ri trancripts for each term, replicate -> insert into final_terms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in phases:\n",
    "    \n",
    "    print('phase = '+phase)\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "        \n",
    "    for rep in reps:\n",
    "        \n",
    "        print('rep = '+rep)\n",
    "                \n",
    "        # Import SEnd mapped trancripts (GFF files)\n",
    "        transcripts = gffpd.read_gff3(GFF_input_filepaths[phase][rep])\n",
    "        #print(transcripts.header)\n",
    "        #display(transcripts.df)\n",
    "\n",
    "        # Unique transcript_id\n",
    "        transcripts.df['transcript_id'] = transcripts.df.index\n",
    "        cols = transcripts.df.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        transcripts.df = transcripts.df[cols]\n",
    "    \n",
    "        # Counts\n",
    "        final_terms_df[phase+'_'+rep+'_t'] = final_terms_df.apply(lambda row: transcript_term_cnt(transcripts.df, row['TTS_position'], row['TTS_strand'], norm_factor)[0], axis=1)\n",
    "        final_terms_df[phase+'_'+rep+'_rt'] = final_terms_df.apply(lambda row: transcript_term_cnt(transcripts.df, row['TTS_position'], row['TTS_strand'], norm_factor)[1], axis=1)\n",
    "        final_terms_df[phase+'_'+rep+'_ri'] = final_terms_df.apply(lambda row: transcript_term_cnt(transcripts.df, row['TTS_position'], row['TTS_strand'], norm_factor)[2], axis=1)\n",
    "        \n",
    "        # Persentages\n",
    "        final_terms_df[phase+'_'+rep+'_pc_t'] = final_terms_df.apply(lambda row: transcript_term_cnt(transcripts.df, row['TTS_position'], row['TTS_strand'], norm_factor)[3], axis=1)\n",
    "        final_terms_df[phase+'_'+rep+'_pc_rt'] = final_terms_df.apply(lambda row: transcript_term_cnt(transcripts.df, row['TTS_position'], row['TTS_strand'], norm_factor)[4], axis=1)\n",
    "        final_terms_df[phase+'_'+rep+'_pc_ri'] = final_terms_df.apply(lambda row: transcript_term_cnt(transcripts.df, row['TTS_position'], row['TTS_strand'], norm_factor)[5], axis=1)\n",
    "\n",
    "    # Add phase averages\n",
    "    def avg_eff_measure(row, reps, phase, measure_suffix):\n",
    "        # measure_suffix: 'pc_t', 'pc_rt', 'pc_ri'\n",
    "        summa = 0\n",
    "        for rep in reps:\n",
    "            summa += row[phase+'_'+rep+'_'+measure_suffix]\n",
    "            \n",
    "        return summa/len(reps)\n",
    "            \n",
    "    ## Average termination    \n",
    "    measure_suffix = 'pc_t'\n",
    "    final_terms_df[phase+'_t_avg'] = final_terms_df.apply(lambda row:  avg_eff_measure(row, reps, phase, measure_suffix), axis=1)\n",
    "    \n",
    "    ## Average readthrough    \n",
    "    measure_suffix = 'pc_rt'\n",
    "    final_terms_df[phase+'_rt_avg'] = final_terms_df.apply(lambda row:  avg_eff_measure(row, reps, phase, measure_suffix), axis=1)\n",
    "        \n",
    "    ## Average reinitiation    \n",
    "    measure_suffix = 'pc_ri'\n",
    "    final_terms_df[phase+'_ri_avg'] = final_terms_df.apply(lambda row:  avg_eff_measure(row, reps, phase, measure_suffix), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize trancript counts: avg_t+rt for each rep, phase; norm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rep average of upstream counts (for normalization)\n",
    "avg_before_cnt = {}\n",
    "\n",
    "for phase in phases:\n",
    "    avg_before_cnt[phase] = {}\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "        \n",
    "    for rep in reps:\n",
    "        avg_before_cnt[phase][rep] = {}\n",
    "        \n",
    "        curr_rep_counts = {} # Counts for for the current replicete (for averaging)\n",
    "        curr_rep_counts['t'] = final_terms_df[phase+'_'+rep+'_t'].tolist() # terminated\n",
    "        curr_rep_counts['rt'] = final_terms_df[phase+'_'+rep+'_rt'].tolist() # read_through\n",
    "        \n",
    "        avg_before_cnt[phase][rep] = (sum(curr_rep_counts['t']) + sum(curr_rep_counts['rt']))/len(curr_rep_counts['t'])\n",
    "        \n",
    "        \n",
    "# Normalization function\n",
    "def norm_tr_cnt(curr_tr_cnt, avg_before_cnt, phase, rep): \n",
    "    return curr_tr_cnt/avg_before_cnt[phase][rep]\n",
    "\n",
    "\n",
    "# Add normilized t, rt, ri counts to final terms_df\n",
    "for phase in phases:\n",
    "\n",
    "    print('phase = '+phase)\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "        \n",
    "    for rep in reps:\n",
    "        \n",
    "        print('rep = '+rep)\n",
    "    \n",
    "        # Normilized counts\n",
    "        final_terms_df[phase+'_'+rep+'_t_norm'] = final_terms_df.apply(lambda row: norm_tr_cnt(row[phase+'_'+rep+'_t'], avg_before_cnt, phase, rep), axis=1)\n",
    "        final_terms_df[phase+'_'+rep+'_rt_norm'] = final_terms_df.apply(lambda row: norm_tr_cnt(row[phase+'_'+rep+'_rt'], avg_before_cnt, phase, rep), axis=1)\n",
    "        final_terms_df[phase+'_'+rep+'_ri_norm'] = final_terms_df.apply(lambda row: norm_tr_cnt(row[phase+'_'+rep+'_ri'], avg_before_cnt, phase, rep), axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "    # Add phase averages\n",
    "    def avg_eff_measure(row, reps, phase, measure_suffix):\n",
    "        # measure_suffix: '_t_norm', '_rt_norm', '_ri_norm'\n",
    "        summa = 0\n",
    "        for rep in reps:\n",
    "            summa += row[phase+'_'+rep+'_'+measure_suffix]\n",
    "            \n",
    "        return summa/len(reps)\n",
    "            \n",
    "    ## Average norm termination    \n",
    "    measure_suffix = 't_norm'\n",
    "    final_terms_df[phase+'_t_norm_avg'] = final_terms_df.apply(lambda row:  avg_eff_measure(row, reps, phase, measure_suffix), axis=1)\n",
    "    \n",
    "    ## Average norm readthrough    \n",
    "    measure_suffix = 'rt_norm'\n",
    "    final_terms_df[phase+'_rt_norm_avg'] = final_terms_df.apply(lambda row:  avg_eff_measure(row, reps, phase, measure_suffix), axis=1)\n",
    "        \n",
    "    ## Average norm reinitiation    \n",
    "    measure_suffix = 'ri_norm'\n",
    "    final_terms_df[phase+'_ri_norm_avg'] = final_terms_df.apply(lambda row:  avg_eff_measure(row, reps, phase, measure_suffix), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add experimntal terms to final_terms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import terminators dataframe\n",
    "exp_terms_df = pd.read_excel(exp_terms_filename, sheet_name='Ju et al. overlap',header=1, names=None, index_col=0, dtype=None, skiprows=0)\n",
    "\n",
    "# Drop unnecesarry columns\n",
    "exp_terms_df = exp_terms_df[['New Name', 'Ju_IT_ID', 'Strands: exp/Ju', 'Comments']]\n",
    "\n",
    "# Rename some columns\n",
    "exp_terms_df = exp_terms_df.rename(columns={'New Name': 'Exp_term_name', 'Ju_IT_ID': 'TTS_id'})\n",
    "print('exp_terms_df:')\n",
    "display(exp_terms_df)\n",
    "\n",
    "# Marge with final_terms_df\n",
    "\n",
    "final_terms_df = pd.merge(final_terms_df, exp_terms_df, how='left', on='TTS_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5M0GipopZ_D"
   },
   "source": [
    "## Graphs and scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show and save the final dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T10:07:02.017618Z",
     "start_time": "2023-05-04T10:07:01.949314Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1660661445312,
     "user": {
      "displayName": "Stepan Denisov",
      "userId": "08382760269110148954"
     },
     "user_tz": -60
    },
    "id": "9yawGlg2pmQF",
    "outputId": "8d429d17-6c57-4e97-bbb9-fd51cd8a576d"
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(final_terms_df)\n",
    "    \n",
    "final_terms_df.to_excel(final_excel_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eff measure vs expression: each phase & replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T14:09:14.455844Z",
     "start_time": "2023-05-04T14:09:10.080453Z"
    }
   },
   "outputs": [],
   "source": [
    "if_color_str_feature = False\n",
    "str_colname = 'Number_of_A_around_5_prime'\n",
    "scale_factor = 1\n",
    "if if_color_str_feature:\n",
    "    print('Dot colour is '+str_colname)\n",
    "\n",
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "expr_colnames = []\n",
    "eff_colnames = []\n",
    "\n",
    "for phase in phases:\n",
    "    if phase == 'log':\n",
    "        for rep in log_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')        \n",
    "                            \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('log_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('log_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                \n",
    "    elif phase == 'stat':\n",
    "        for rep in stat_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "                \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('stat_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "   \n",
    "            \n",
    "           \n",
    "fig_s, ax_s = plt.subplots(rep_number,2)\n",
    "fig_s.set_size_inches(7*2,7*rep_number)\n",
    "fig_s.suptitle('Terminator '+eff_measure+' vs. expression: '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "for i in range(rep_number):\n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df[[expr_colname, eff_colname, str_colname]].dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "    expressions = curr_final_terms_df[expr_colname].tolist()\n",
    "    efficiencies = curr_final_terms_df[eff_colname].tolist()\n",
    "    STRs = curr_final_terms_df[str_colname].tolist() # structural feature\n",
    "    #print(np.array(STRs)*scale_factor)\n",
    "    \n",
    "    if is_expr_norm:\n",
    "        \n",
    "        ax_s[i,0].set_title(eff_colname, fontsize=10)\n",
    "        \n",
    "        if if_color_str_feature:\n",
    "            pc = ax_s[i,0].scatter(expressions, efficiencies, s=10, c=np.array(STRs)*scale_factor)\n",
    "        else:\n",
    "            pc = ax_s[i,0].scatter(expressions, efficiencies, s=4)\n",
    "        \n",
    "\n",
    "        ax_s[i,0].set_xlabel('Expression')\n",
    "        ax_s[i,0].set_ylabel('Terminator '+eff_measure+ ' rate')\n",
    "\n",
    "        # Correlations\n",
    "        r, p = scipy.stats.pearsonr(expressions, efficiencies)\n",
    "        rho, prho = scipy.stats.spearmanr(expressions, efficiencies)\n",
    "        tau, ptau = scipy.stats.kendalltau(expressions, efficiencies)\n",
    "        #ax_s[i,0].annotate('    Person R = '+ str(r.round(1)) + ', p = '+ str(p.round(2))+ '\\n    Spearman rho =  '+ str(rho.round(1))+ ', p = '+ str(prho.round(4))+ '\\n    Kendall tau =  '+ str(tau.round(1))+ ', p = '+ str(ptau.round(4)), (1, 0))\n",
    "        if (eff_measure == 'efficiency') or (eff_measure == 'termination'):\n",
    "            ax_s[i,0].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (0.2, 0))\n",
    "        else:\n",
    "            ax_s[i,0].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (7, 0.8))\n",
    "       \n",
    "\n",
    "        ax_s[i,0].set_xlim(-0.5, max_expr_th+0.5)\n",
    "        ax_s[i,0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[i,0].set_xticks(np.arange(0, max_expr_th, step=0.5))\n",
    "        #ax_s[i,0].set_xticklabels(np.arange(0, max_expr_th, step=0.5))\n",
    "\n",
    "        ax_s[i,0].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_s[i,0].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "\n",
    "        ax_s[i,1].set_title(eff_colname, fontsize=10)\n",
    "        ax_s[i,1].set_xscale('log')\n",
    "\n",
    "        #ax_s[i,1].scatter(expressions, efficiencies, s=6)\n",
    "        if if_color_str_feature:\n",
    "            pc = ax_s[i,1].scatter(expressions, efficiencies, s=10, c=np.array(STRs)*scale_factor)\n",
    "        else:\n",
    "            pc = ax_s[i,1].scatter(expressions, efficiencies, s=4)\n",
    "\n",
    "        ax_s[i,1].set_xlabel('Expression')\n",
    "        ax_s[i,1].set_ylabel('Terminator '+eff_measure+ ' rate')\n",
    "\n",
    "        #ax_s[i,1].set_xlim(1, 100000)\n",
    "        ax_s[i,1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[i,1].set_xticks(np.arange(0, 10000, step=1000))\n",
    "        #ax_s[i,1].set_xticklabels(np.arange(0, 10000, step=1000))\n",
    "\n",
    "        ax_s[i,1].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_s[i,1].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "        \n",
    "        # Colorbar\n",
    "        if if_color_str_feature:\n",
    "            cax = ax_s[i,1].inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax_s[i,1].transAxes)\n",
    "            colorbar = fig_s.colorbar(pc, cax=cax)\n",
    "            colorbar.set_label(str_colname, rotation=270, va=\"baseline\")\n",
    "\n",
    "\n",
    "fig_s.show()\n",
    "fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_rates_reps_phases.pdf', bbox_inches='tight')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upstream expression vs downstraem expression: each phase & replicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dn_exp_wig = True # True - expression is mease on wig files as in  Eff measure vs expression: each phase & replicate\n",
    "# Make sense only for rt and ri\n",
    "if eff_measure == 'readthrough' or eff_measure == 'reinitiation':\n",
    "    if_color_str_feature = False\n",
    "    str_colname = 'Number_of_A_around_5_prime'\n",
    "    scale_factor = 1\n",
    "    if if_color_str_feature:\n",
    "        print('Dot colour is '+str_colname)\n",
    "\n",
    "    rep_number = len(log_reps) + len(stat_reps)\n",
    "    all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "    up_expr_colnames = []\n",
    "    dn_expr_colnames = []\n",
    "    \n",
    "    expr_colnames = []\n",
    "\n",
    "    for phase in phases:\n",
    "        if phase == 'log':\n",
    "            for rep in log_reps:\n",
    "\n",
    "                up_expr_colnames.append([phase+'_'+rep+'_t_norm', phase+'_'+rep+'_rt_norm'])        \n",
    "\n",
    "                if eff_measure == 'readthrough':\n",
    "                    dn_expr_colnames.append(phase+'_'+rep+'_rt_norm')\n",
    "                elif eff_measure == 'reinitiation':\n",
    "                    dn_expr_colnames.append(phase+'_'+rep+'_ri_norm')\n",
    "                else: \n",
    "                    raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                    \n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')\n",
    "\n",
    "        elif phase == 'stat':\n",
    "            for rep in stat_reps:\n",
    "\n",
    "                up_expr_colnames.append([phase+'_'+rep+'_t_norm', phase+'_'+rep+'_rt_norm'])        \n",
    "\n",
    "                if eff_measure == 'readthrough':\n",
    "                    dn_expr_colnames.append(phase+'_'+rep+'_rt_norm')\n",
    "                elif eff_measure == 'reinitiation':\n",
    "                    dn_expr_colnames.append(phase+'_'+rep+'_ri_norm')\n",
    "                else: \n",
    "                    raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                    \n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "\n",
    "\n",
    "\n",
    "    fig_s, ax_s = plt.subplots(rep_number,2)\n",
    "    fig_s.set_size_inches(7*2,7*rep_number)\n",
    "    fig_s.suptitle('Terminator upstream expr (t+rt) vs.downstream expression ('+eff_measure+'): '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "    for i in range(rep_number):\n",
    "\n",
    "        up_expr_colname = up_expr_colnames[i]\n",
    "        dn_expr_colname = dn_expr_colnames[i]\n",
    "        \n",
    "        expr_colname = expr_colnames[i]\n",
    "        \n",
    "        if is_dn_exp_wig: # old style expression\n",
    "            up_expr_colname = expr_colname\n",
    "            all_colnames = list(set([up_expr_colname] + [dn_expr_colname] + [str_colname]))\n",
    "        else:\n",
    "            all_colnames = list(set(up_expr_colname + [dn_expr_colname] + [str_colname]))\n",
    "            \n",
    "        #print(all_colnames)\n",
    "        # Drop NA values (and ones?)\n",
    "        if is_dn_exp_wig: # old style expression\n",
    "            curr_final_terms_df = final_terms_df[all_colnames].dropna(how='any', subset=[up_expr_colname, dn_expr_colname])\n",
    "        else:\n",
    "            curr_final_terms_df = final_terms_df[all_colnames].dropna(how='any', subset=[up_expr_colname[0], dn_expr_colname])\n",
    "\n",
    "        # Delelte dn_expr = 1?\n",
    "        if is_eff_1_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[dn_expr_colname]!=1]\n",
    "\n",
    "        # Delelte dn_expr = 0?\n",
    "        if is_eff_0_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[dn_expr_colname]!=0]\n",
    "\n",
    "        # Delete negative dn_expr?\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[dn_expr_colname] >= 0]\n",
    "\n",
    "        # Delete dn_expr > max_eff_th\n",
    "        #curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[dn_expr_colname] <= max_eff_th]\n",
    "\n",
    "\n",
    "        # Delete expressions < treshold\n",
    "        if is_neg_eff_deleted:\n",
    "            if is_dn_exp_wig: # old style expression\n",
    "                curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[up_expr_colname] >= min_expr_th]\n",
    "                curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[up_expr_colname] <= max_expr_th]\n",
    "            else:\n",
    "                curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[up_expr_colname[0]]+curr_final_terms_df[up_expr_colname[1]] >= min_expr_th]\n",
    "                curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[up_expr_colname[0]]+curr_final_terms_df[up_expr_colname[1]] <= max_expr_th]\n",
    "\n",
    "        up_expressions = []\n",
    "        if  is_dn_exp_wig: # old style expression\n",
    "            up_expressions = curr_final_terms_df[up_expr_colname].tolist()\n",
    "        else:\n",
    "            up_expressions_t = curr_final_terms_df[up_expr_colname[0]].tolist()\n",
    "            up_expressions_rt = curr_final_terms_df[up_expr_colname[1]].tolist()\n",
    "            assert len(up_expressions_t) == len(up_expressions_rt)\n",
    "\n",
    "            for u_e in range(len(up_expressions_t)):\n",
    "                up_expressions.append(up_expressions_t[u_e]+up_expressions_rt[u_e]) \n",
    "        \n",
    "        dn_expressions = curr_final_terms_df[dn_expr_colname].tolist()\n",
    "        assert len(up_expressions) == len(dn_expressions)\n",
    "        STRs = curr_final_terms_df[str_colname].tolist() # structural feature\n",
    "        #print(np.array(STRs)*scale_factor)\n",
    "\n",
    "\n",
    "\n",
    "        ax_s[i,0].set_title(dn_expr_colname, fontsize=10)\n",
    "\n",
    "        if if_color_str_feature:\n",
    "            pc = ax_s[i,0].scatter(up_expressions, dn_expressions, s=10, c=np.array(STRs)*scale_factor)\n",
    "        else:\n",
    "            pc = ax_s[i,0].scatter(up_expressions, dn_expressions, s=4)\n",
    "\n",
    "\n",
    "        if is_dn_exp_wig:\n",
    "            ax_s[i,0].set_xlabel('Upstream expression: wig')\n",
    "        else:\n",
    "            ax_s[i,0].set_xlabel('Upstream expression: t+rt')\n",
    "        ax_s[i,0].set_ylabel('Downstraem expression: '+eff_measure)\n",
    "\n",
    "        # Correlations\n",
    "        #print(up_expressions)\n",
    "        #print(dn_expressions)\n",
    "        if len(up_expressions)>=2 and len(up_expressions)>=2:\n",
    "            r, p = scipy.stats.pearsonr(up_expressions, dn_expressions)\n",
    "            rho, prho = scipy.stats.spearmanr(up_expressions, dn_expressions)\n",
    "            tau, ptau = scipy.stats.kendalltau(up_expressions, dn_expressions)\n",
    "        else:\n",
    "            r = 0\n",
    "            p = 1\n",
    "            rho = 0 \n",
    "            prho = 1\n",
    "            tau = 0\n",
    "            ptau = 1\n",
    "\n",
    "        ax_s[i,0].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (7, 0.8))\n",
    "\n",
    "\n",
    "        ax_s[i,0].set_xlim(-0.5, max_expr_th+0.5)\n",
    "        ax_s[i,0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[i,0].set_xticks(np.arange(0, max_expr_th, step=0.5))\n",
    "        #ax_s[i,0].set_xticklabels(np.arange(0, max_expr_th, step=0.5))\n",
    "\n",
    "        ax_s[i,0].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_s[i,0].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "\n",
    "        ax_s[i,1].set_title(dn_expr_colname, fontsize=10)\n",
    "        ax_s[i,1].set_xscale('log')\n",
    "\n",
    "        #ax_s[i,1].scatter(expressions, efficiencies, s=6)\n",
    "        if if_color_str_feature:\n",
    "            pc = ax_s[i,1].scatter(up_expressions, dn_expressions, s=10, c=np.array(STRs)*scale_factor)\n",
    "        else:\n",
    "            pc = ax_s[i,1].scatter(up_expressions, dn_expressions, s=4)\n",
    "\n",
    "        ax_s[i,1].set_xlabel('Up expr: t+rt')\n",
    "        ax_s[i,1].set_ylabel('Dn expr: '+eff_measure)\n",
    "\n",
    "        ax_s[i,1].set_xlim(0.1, 20)\n",
    "        ax_s[i,1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[i,1].set_xticks(np.arange(0, 10000, step=1000))\n",
    "        #ax_s[i,1].set_xticklabels(np.arange(0, 10000, step=1000))\n",
    "\n",
    "        ax_s[i,1].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_s[i,1].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "        # Colorbar\n",
    "        if if_color_str_feature:\n",
    "            cax = ax_s[i,1].inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax_s[i,1].transAxes)\n",
    "            colorbar = fig_s.colorbar(pc, cax=cax)\n",
    "            colorbar.set_label(str_colname, rotation=270, va=\"baseline\")\n",
    "\n",
    "\n",
    "\n",
    "    fig_s.show()\n",
    "    fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_cnts_reps_phases.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each phase seperately (replicate averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_color_str_feature = False\n",
    "str_colname = 'Number_of_A_around_5_prime'\n",
    "scale_factor = 1\n",
    "if if_color_str_feature:\n",
    "    print('Dot colour is '+str_colname)\n",
    "\n",
    "for is_counts in [True, False]:# True: Use normalised rt, ri counts intead of proportions(rates)\n",
    "\n",
    "    expr_colnames = []\n",
    "    eff_colnames = []\n",
    "\n",
    "\n",
    "    for phase in phases:\n",
    "        expr_colnames.append(phase+'_avg_norm_expr')\n",
    "\n",
    "        if eff_measure == 'efficiency':\n",
    "            eff_colnames.append(phase+'_avg_eff')\n",
    "        elif eff_measure == 'readthrough':\n",
    "            if is_counts:\n",
    "                eff_colnames.append(phase+'_rt_norm_avg')\n",
    "            else:\n",
    "                eff_colnames.append(phase+'_rt_avg')\n",
    "        elif eff_measure == 'reinitiation':\n",
    "            if is_counts:\n",
    "                eff_colnames.append(phase+'_ri_norm_avg')\n",
    "            else:\n",
    "                eff_colnames.append(phase+'_ri_avg')\n",
    "        elif eff_measure == 'termination':\n",
    "            eff_colnames.append(phase+'_t_avg')\n",
    "        else: \n",
    "            raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "\n",
    "    fig_s, ax_s = plt.subplots(len(phases),2)\n",
    "    fig_s.set_size_inches(7*2,7*len(phases))\n",
    "    fig_s.suptitle('Terminator '+eff_measure+' vs. expression: '+experiment_set+', replicate averages')\n",
    "\n",
    "    for i in range(len(phases)):\n",
    "\n",
    "        expr_colname = expr_colnames[i]\n",
    "        eff_colname = eff_colnames[i]\n",
    "        # Drop NA values (and ones?)\n",
    "        curr_final_terms_df = final_terms_df[[expr_colname, eff_colname, str_colname]].dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "\n",
    "        # Delelte eff = 1?\n",
    "        if is_eff_1_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "\n",
    "        # Delelte eff = 0?\n",
    "        if is_eff_0_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "\n",
    "        # Delete negative efficiencies?\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "\n",
    "        # Delete efficiencises > max_eff_th\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "\n",
    "        # Delete expressions < treshold\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "\n",
    "        expressions = curr_final_terms_df[expr_colname].tolist()\n",
    "        efficiencies = curr_final_terms_df[eff_colname].tolist()\n",
    "        STRs = curr_final_terms_df[str_colname].tolist() # structural feature\n",
    "\n",
    "\n",
    "        ax_s[i,0].set_title(eff_colname, fontsize=10)\n",
    "\n",
    "        if if_color_str_feature:\n",
    "            pc = ax_s[i,0].scatter(expressions, efficiencies, s=10, c=np.array(STRs)*scale_factor)\n",
    "        else:\n",
    "            pc = ax_s[i,0].scatter(expressions, efficiencies, s=4)\n",
    "\n",
    "        ax_s[i,0].set_xlabel('Expression')\n",
    "        ax_s[i,0].set_ylabel('Terminator '+eff_measure+' rate')\n",
    "\n",
    "        # Correlations\n",
    "        r, p = scipy.stats.pearsonr(expressions, efficiencies)\n",
    "        rho, prho = scipy.stats.spearmanr(expressions, efficiencies)\n",
    "        tau, ptau = scipy.stats.kendalltau(expressions, efficiencies)\n",
    "        #ax_s[i,0].annotate('    Person R = '+ str(r.round(1)) + ', p = '+ str(p.round(2))+ '\\n    Spearman rho =  '+ str(rho.round(1))+ ', p = '+ str(prho.round(4))+ '\\n    Kendall tau =  '+ str(tau.round(1))+ ', p = '+ str(ptau.round(4)), (1, 0))\n",
    "        if (eff_measure == 'efficiency') or (eff_measure == 'termination'):\n",
    "            ax_s[i,0].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (5, 0))\n",
    "        else:\n",
    "            ax_s[i,0].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (7, 0.8))\n",
    "\n",
    "\n",
    "\n",
    "        ax_s[i,0].set_xlim(-0.5, max_expr_th+0.5)\n",
    "        ax_s[i,0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[i,0].set_xticks(np.arange(0, max_expr_th, step=0.5))\n",
    "        #ax_s[i,0].set_xticklabels(np.arange(0, max_expr_th, step=0.5))\n",
    "\n",
    "        ax_s[i,0].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_s[i,0].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "\n",
    "        ax_s[i,1].set_title(eff_colname, fontsize=10)\n",
    "        ax_s[i,1].set_xscale('log')\n",
    "\n",
    "        if if_color_str_feature:\n",
    "            pc = ax_s[i,1].scatter(expressions, efficiencies, s=10, c=np.array(STRs)*scale_factor)\n",
    "        else:\n",
    "            pc = ax_s[i,1].scatter(expressions, efficiencies, s=4)\n",
    "\n",
    "        ax_s[i,1].set_xlabel('Expression')\n",
    "        ax_s[i,1].set_ylabel('Terminator '+eff_measure+' rate')\n",
    "\n",
    "        #ax_s[i,1].set_xlim(-0.05, 100000)\n",
    "        ax_s[i,1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[i,1].set_xticks(np.arange(0, 10000, step=1000))\n",
    "        #ax_s[i,1].set_xticklabels(np.arange(0, 10000, step=1000))\n",
    "\n",
    "        ax_s[i,1].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_s[i,1].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "        # Colorbar\n",
    "        if if_color_str_feature:\n",
    "            cax = ax_s[i,1].inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax_s[i,1].transAxes)\n",
    "            colorbar = fig_s.colorbar(pc, cax=cax)\n",
    "            colorbar.set_label(str_colname, rotation=270, va=\"baseline\")\n",
    "\n",
    "\n",
    "    fig_s.show()\n",
    "    if is_counts:\n",
    "        fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'cnts_phases.pdf', bbox_inches='tight')  \n",
    "    else:\n",
    "        fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_rates_phases.pdf', bbox_inches='tight')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes between phases (replicate averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect avg expression & avg efficiencies data\n",
    "\n",
    "for is_counts in [True, False]:# True: Use normalised rt, ri counts intead of proportions(rates)\n",
    "    \n",
    "    expr_colnames = []\n",
    "    eff_colnames = []\n",
    "\n",
    "    for phase in phases:\n",
    "        expr_colnames.append(phase+'_avg_norm_expr')\n",
    "\n",
    "        if eff_measure == 'efficiency':\n",
    "            eff_colnames.append(phase+'_avg_eff')\n",
    "        elif eff_measure == 'readthrough':\n",
    "            if is_counts:\n",
    "                eff_colnames.append(phase+'_rt_norm_avg')\n",
    "            else:\n",
    "                eff_colnames.append(phase+'_rt_avg')\n",
    "        elif eff_measure == 'reinitiation':\n",
    "            if is_counts:\n",
    "                eff_colnames.append(phase+'_ri_norm_avg')\n",
    "            else:\n",
    "                eff_colnames.append(phase+'_ri_avg')\n",
    "        elif eff_measure == 'termination':\n",
    "            eff_colnames.append(phase+'_t_avg')\n",
    "        else: \n",
    "            raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "    # To calculate differences later \n",
    "    all_expressions = []\n",
    "    all_efficiencies = []\n",
    "\n",
    "    # Filter final_terms_df\n",
    "    ## Drop NA values\n",
    "\n",
    "    # Dot size parameters\n",
    "    is_dot_size_expression = True\n",
    "    scale_factor = 100\n",
    "    str_colname = 'dG'\n",
    "    curr_final_terms_df = final_terms_df[expr_colnames + eff_colnames +[str_colname]].dropna(how='any')\n",
    "\n",
    "    ## Apply other filters \n",
    "    for i in range(len(phases)):\n",
    "\n",
    "        expr_colname = expr_colnames[i]\n",
    "        eff_colname = eff_colnames[i]\n",
    "\n",
    "        ### Delelte eff = 1?\n",
    "        if is_eff_1_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "\n",
    "        ### Delelte eff = 0?\n",
    "        if is_eff_0_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "\n",
    "        ### Delete negative efficiencies?\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "\n",
    "        ### Delete efficiencises > max_eff_th\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "\n",
    "        ### Delete expressions < treshold\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "\n",
    "    STRs = curr_final_terms_df[str_colname].tolist() # structural feature\n",
    "    print('Dot size is '+str_colname)        \n",
    "\n",
    "    for i in range(len(phases)):\n",
    "\n",
    "        expr_colname = expr_colnames[i]\n",
    "        eff_colname = eff_colnames[i]\n",
    "\n",
    "        expressions = curr_final_terms_df[expr_colname].tolist()\n",
    "        efficiencies = curr_final_terms_df[eff_colname].tolist()\n",
    "\n",
    "        all_expressions.append(expressions)\n",
    "        all_efficiencies.append(efficiencies)\n",
    "\n",
    "    #============================================================================================    \n",
    "    # Plot avg expression in stat phase vs avg expr in log phase\n",
    "\n",
    "    ## Draw scatterplots\n",
    "\n",
    "    fig_e, ax_e = plt.subplots(1,2)\n",
    "    fig_e.set_size_inches(7*2,7*1)\n",
    "    fig_e.suptitle('Avg expression between phases: '+experiment_set+', replicate averages')\n",
    "\n",
    "    if is_expr_norm:\n",
    "        ax_e[0].set_title('Expressions between 0 and 1', fontsize=10)\n",
    "\n",
    "        ax_e[0].scatter(log_expressions, stat_expressions, s=4)\n",
    "\n",
    "        ## Add 1-1 line to scatterplot \n",
    "        ax_e[0].plot(np.array(log_expressions), np.array(log_expressions))\n",
    "\n",
    "        ax_e[0].set_xlabel('Log phase expression')\n",
    "        ax_e[0].set_ylabel('Stat phase expression')\n",
    "\n",
    "\n",
    "        ax_e[0].set_xlim(-0.1, 1.1)\n",
    "        ax_e[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "\n",
    "        ax_e[0].set_xticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_e[0].set_xticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "        ax_e[0].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "        ax_e[0].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "\n",
    "        ax_e[1].set_title('All expressions', fontsize=10)\n",
    "        #ax_e[1].set_xscale('log')\n",
    "\n",
    "        ax_e[1].scatter(log_expressions, stat_expressions, s=6)\n",
    "\n",
    "        ## Add 1-1 line to scatterplot \n",
    "        ax_e[1].plot(np.array(log_expressions), np.array(log_expressions))\n",
    "\n",
    "         # Correlations\n",
    "        #r, p = scipy.stats.pearsonr(log_expressions, stat_expressions)\n",
    "        #rho, prho = scipy.stats.spearmanr(log_expressions, stat_expressions)\n",
    "        #tau, ptau = scipy.stats.kendalltau(log_expressions, stat_expressions)\n",
    "        #ax_e[0].annotate('    Person R = '+ str(r.round(1)) + ', p = '+ str(p.round(2))+ '\\n    Spearman rho =  '+ str(rho.round(1))+ ', p = '+ str(prho.round(4))+ '\\n    Kendall tau =  '+ str(tau.round(1))+ ', p = '+ str(ptau.round(4)), (1, 0.7))\n",
    "        #ax_e[1].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (6, 5))\n",
    "\n",
    "\n",
    "        ax_e[1].set_xlabel('Log phase expression')\n",
    "        ax_e[1].set_ylabel('Stat phase expression')\n",
    "\n",
    "        #ax_e[1].set_xlim(-10000, 10000)\n",
    "        #ax_e[1].set_ylim(-1.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_e[1].set_xticks(np.arange(0, 10000, step=1000))\n",
    "        #ax_e[1].set_xticklabels(np.arange(0, 10000, step=1000))\n",
    "\n",
    "        #ax_e[1].set_yticks(np.arange(0, 10, step=1))\n",
    "        #ax_e[1].set_yticklabels(np.arange(0, 10, step=1).round(decimals=1))\n",
    "\n",
    "    fig_e.show()\n",
    "    \n",
    "    if is_counts:\n",
    "        fig_e.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_cnts_stat_log_expr.pdf', bbox_inches='tight')\n",
    "    else:\n",
    "        fig_e.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_rates_stat_log_expr.pdf', bbox_inches='tight')\n",
    "\n",
    "    #============================================================================================        \n",
    "\n",
    "    # Calculate differencies\n",
    "    ## Efficiency diffs\n",
    "    eff_with_phases = list(map(list, zip(*all_efficiencies))) # Transpose list of lists\n",
    "    log_phase_effs = []\n",
    "    eff_diffs = []\n",
    "\n",
    "    for phase_eff in eff_with_phases:\n",
    "        log_phase_effs.append(phase_eff[0])\n",
    "        eff_dif = phase_eff[1] - phase_eff[0] # stat - log\n",
    "        eff_diffs.append(eff_dif)\n",
    "\n",
    "    ## Expression diffs\n",
    "    expr_with_phases = list(map(list, zip(*all_expressions))) # Transpose list of lists\n",
    "    log_phase_exprs = []\n",
    "    expr_diffs = []\n",
    "\n",
    "    for phase_expr in expr_with_phases:\n",
    "        log_phase_exprs.append(phase_expr[0])\n",
    "        expr_dif = phase_expr[1] - phase_expr[0]  # stat - log\n",
    "        expr_diffs.append(expr_dif)\n",
    "\n",
    "    # Draw sctterplots\n",
    "\n",
    "    fig_s, ax_s = plt.subplots(1,2)\n",
    "    fig_s.set_size_inches(7*2,7*1)\n",
    "    fig_s.suptitle('Terminator '+eff_measure+' difference vs. expression diff (between phases): '+experiment_set+', replicate averages')\n",
    "\n",
    "    if is_expr_norm:\n",
    "\n",
    "        ax_s[0].set_title('Expression diffs between -2.5 and +2.5', fontsize=10)\n",
    "\n",
    "        if is_dot_size_expression:\n",
    "            pc = ax_s[0].scatter(expr_diffs, eff_diffs, s=np.array(log_phase_exprs)*scale_factor, c=np.log10(log_phase_effs))\n",
    "        else:                              \n",
    "            pc = ax_s[0].scatter(expr_diffs, eff_diffs, s=np.array(STRs)*scale_factor, c=np.log10(log_phase_effs))\n",
    "\n",
    "        ## Obtain m (slope) and b(intercept) of linear regression line\n",
    "        m, b = np.polyfit(np.array(expr_diffs), np.array(eff_diffs), 1)\n",
    "\n",
    "        ## Add linear regression line to scatterplot \n",
    "        ax_s[0].plot(np.array(expr_diffs), m*np.array(expr_diffs)+b)\n",
    "\n",
    "        ax_s[0].set_xlabel('Expression stat - Expression log')\n",
    "        ax_s[0].set_ylabel(eff_measure+' stat - '+eff_measure+' log')\n",
    "\n",
    "\n",
    "\n",
    "        ax_s[0].set_xlim(-2.5, 2.5)\n",
    "        ax_s[0].set_ylim(-1.05, 1.05)\n",
    "\n",
    "\n",
    "        ax_s[0].set_xticks(np.arange(-2.5, 2.5, step=0.5))\n",
    "        ax_s[0].set_xticklabels(np.arange(-2.5, 2.5, step=0.5))\n",
    "\n",
    "        ax_s[0].set_yticks(np.arange(-1, 1.1, step=0.1))\n",
    "        ax_s[0].set_yticklabels(np.arange(-1, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "\n",
    "        ax_s[1].set_title('All diffs', fontsize=10)\n",
    "        #ax_s[1].set_xscale('log')\n",
    "\n",
    "        if is_dot_size_expression:\n",
    "            scat = ax_s[1].scatter(expr_diffs, eff_diffs, s=np.array(log_phase_exprs)*scale_factor, c=np.log10(log_phase_effs))\n",
    "        else:\n",
    "            scat = ax_s[1].scatter(expr_diffs, eff_diffs, s=np.array(STRs)*scale_factor, c=np.log10(log_phase_effs))\n",
    "\n",
    "\n",
    "        # Dot Size legend\n",
    "        # produce a legend with a cross-section of sizes from the scatter\n",
    "        handles, labels = scat.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "        ## Only 3 dots on the legend\n",
    "        mid_0_based_index = round(len(handles)/2)-1\n",
    "        labels = ['$\\mathdefault{1}$', '$\\mathdefault{'+str(mid_0_based_index+1)+'}$', '$\\mathdefault{'+str(len(handles))+'}$']\n",
    "        handles = [handles[0],handles[mid_0_based_index],handles[-1]]\n",
    "        legend2 = ax_s[1].legend(handles, labels, loc=\"lower left\", title=\"Log phase Expression \",labelspacing=2)\n",
    "\n",
    "        # Colorbar legend\n",
    "        cax = ax_s[1].inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax_s[1].transAxes)\n",
    "        colorbar = fig_s.colorbar(pc, cax=cax)\n",
    "        colorbar.set_label(\"log10(Log phase \"+eff_measure+\")\", rotation=270, va=\"baseline\")\n",
    "\n",
    "        ## Obtain m (slope) and b(intercept) of linear regression line\n",
    "        m, b = np.polyfit(np.array(expr_diffs), np.array(eff_diffs), 1)\n",
    "\n",
    "        ## Add linear regression line to scatterplot \n",
    "        ax_s[1].plot(np.array(expr_diffs), m*np.array(expr_diffs)+b)\n",
    "\n",
    "        # Correlations\n",
    "        r, p = scipy.stats.pearsonr(expr_diffs, eff_diffs)\n",
    "        rho, prho = scipy.stats.spearmanr(expr_diffs, eff_diffs)\n",
    "        tau, ptau = scipy.stats.kendalltau(expr_diffs, eff_diffs)\n",
    "        ax_s[1].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (-2, 0.7))\n",
    "\n",
    "\n",
    "        ax_s[1].set_xlabel('Expression stat - Expression log')\n",
    "        ax_s[1].set_ylabel(eff_measure+' stat - '+eff_measure+' log')\n",
    "\n",
    "        #ax_s[1].set_xlim(-10000, 10000)\n",
    "        ax_s[1].set_ylim(-1.05, 1.05)\n",
    "\n",
    "\n",
    "        #ax_s[1].set_xticks(np.arange(0, 10000, step=1000))\n",
    "        #ax_s[1].set_xticklabels(np.arange(0, 10000, step=1000))\n",
    "\n",
    "        ax_s[1].set_yticks(np.arange(-1, 1.1, step=0.1))\n",
    "        ax_s[1].set_yticklabels(np.arange(-1, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "    fig_s.show()\n",
    "    \n",
    "    if is_counts:\n",
    "        fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_cnts_diff.pdf', bbox_inches='tight')\n",
    "    else:\n",
    "        fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_rates_diff.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes between phases & replicates (all vs all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "\n",
    "\n",
    "for is_counts in [True, False]:# True: Use normalised rt, ri counts intead of proportions(rates)\n",
    "    \n",
    "    all_reps_phases = []\n",
    "    expr_colnames = []\n",
    "    eff_colnames = []\n",
    "\n",
    "    if is_expr_norm:\n",
    "        for phase in phases:\n",
    "            if phase == 'log':\n",
    "                for rep in log_reps:\n",
    "                    expr_colnames.append('log_'+rep+'_norm_expr')\n",
    "\n",
    "                    if eff_measure == 'efficiency':\n",
    "                        eff_colnames.append('log_'+rep+'_eff')\n",
    "\n",
    "                    elif eff_measure == 'readthrough':\n",
    "                        if is_counts:\n",
    "                            eff_colnames.append('log_'+rep+'_rt_norm')\n",
    "                        else:\n",
    "                            eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "\n",
    "                    elif eff_measure == 'reinitiation':\n",
    "                        if is_counts:\n",
    "                            eff_colnames.append('log_'+rep+'_ri_norm')\n",
    "                        else:\n",
    "                            eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "\n",
    "                    elif eff_measure == 'termination':\n",
    "                        eff_colnames.append('log_'+rep+'_pc_t')\n",
    "                    else: \n",
    "                        raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "                    all_reps_phases.append('log_'+rep)\n",
    "\n",
    "            elif phase == 'stat':\n",
    "                for rep in stat_reps:\n",
    "                    expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "\n",
    "                    if eff_measure == 'efficiency':\n",
    "                        eff_colnames.append('stat_'+rep+'_eff')\n",
    "\n",
    "                    elif eff_measure == 'readthrough':\n",
    "                        if is_counts:\n",
    "                            eff_colnames.append('stat_'+rep+'_ri_norm')\n",
    "                        else:\n",
    "                            eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "\n",
    "                    elif eff_measure == 'reinitiation':\n",
    "                        if is_counts:\n",
    "                            eff_colnames.append('stat_'+rep+'_ri_norm')\n",
    "                        else:\n",
    "                            eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "\n",
    "                    elif eff_measure == 'termination':\n",
    "                        eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "                    else: \n",
    "                        raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "                    all_reps_phases.append('stat_'+rep)\n",
    "\n",
    "\n",
    "    ## Drop NA values\n",
    "    #curr_final_terms_df = final_terms_df[expr_colnames + eff_colnames].dropna(how='any')  \n",
    "    curr_final_terms_df = final_terms_df\n",
    "\n",
    "    all_expressions = []\n",
    "    all_efficiencies = []\n",
    "    for i in range(rep_number):\n",
    "\n",
    "        expr_colname = expr_colnames[i]\n",
    "        eff_colname = eff_colnames[i]\n",
    "\n",
    "        expressions = curr_final_terms_df[expr_colname].tolist()\n",
    "        efficiencies = curr_final_terms_df[eff_colname].tolist()\n",
    "\n",
    "        all_expressions.append(expressions)\n",
    "        all_efficiencies.append(efficiencies)\n",
    "\n",
    "    # Calculate differencies\n",
    "    ## Efficiency diffs\n",
    "    eff_with_reps_phases = list(map(list, zip(*all_efficiencies))) # Transpose list of lists\n",
    "    eff_diffs = []\n",
    "    log_phase_effs = []\n",
    "\n",
    "    ## Expression diffs\n",
    "    expr_with_reps_phases = list(map(list, zip(*all_expressions))) # Transpose list of lists\n",
    "    expr_diffs = []\n",
    "    log_phase_exprs = []\n",
    "\n",
    "    if len(eff_with_reps_phases) != len(expr_with_reps_phases):\n",
    "        raise TypeError('FIX IT!')\n",
    "\n",
    "\n",
    "    for i in range(rep_number): # rows\n",
    "        log_phase_effs.append([])\n",
    "        eff_diffs.append([])\n",
    "\n",
    "        log_phase_exprs.append([])\n",
    "        expr_diffs.append([])\n",
    "\n",
    "        for j in range(rep_number): # columns\n",
    "            log_phase_effs[i].append([])\n",
    "            eff_diffs[i].append([])\n",
    "\n",
    "            log_phase_exprs[i].append([])\n",
    "            expr_diffs[i].append([])\n",
    "\n",
    "            for k in range(len(eff_with_reps_phases)):\n",
    "                rep_phase_eff = eff_with_reps_phases[k]\n",
    "                rep_phase_expr = expr_with_reps_phases[k]\n",
    "\n",
    "                # Filter out NAs\n",
    "                if (np.isnan(rep_phase_eff[i]) or np.isnan(rep_phase_eff[j]) or np.isnan(rep_phase_expr[i]) or np.isnan(rep_phase_expr[j])):\n",
    "                    continue\n",
    "                log_phase_effs[i][j].append(rep_phase_eff[j])\n",
    "                eff_dif = rep_phase_eff[i] - rep_phase_eff[j] # row - column\n",
    "                eff_diffs[i][j].append(eff_dif)\n",
    "\n",
    "                log_phase_exprs[i][j].append(rep_phase_expr[j])\n",
    "                expr_dif = rep_phase_expr[i] - rep_phase_expr[j] # row - column\n",
    "                expr_diffs[i][j].append(expr_dif)\n",
    "\n",
    "    # Draw expression vs expression scatterplots\n",
    "\n",
    "    fig_e, ax_e = plt.subplots(rep_number,rep_number)\n",
    "    fig_e.set_size_inches(7*rep_number,7*rep_number)\n",
    "    fig_e.suptitle('Expression vs. expression: '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "    for i in range(rep_number): # rows\n",
    "        for j in range(rep_number): # columns\n",
    "\n",
    "            if is_expr_norm:\n",
    "\n",
    "                ax_e[i][j].set_title(all_reps_phases[i]+' vs '+all_reps_phases[j] , fontsize=10)\n",
    "                \n",
    "                ax_e[i][j].scatter(all_expressions[i], all_expressions[j], s=4)\n",
    "\n",
    "                if i != j:\n",
    "                    ## 1-1 line\n",
    "                    m, b = np.polyfit(np.array(all_expressions[i]), np.array(all_expressions[i]), 1)\n",
    "\n",
    "                    ## Add 1-1 line to scatterplot \n",
    "                    ax_e[i][j].plot(np.array(all_expressions[i]), m*np.array(all_expressions[i])+b)\n",
    "\n",
    "                ax_e[i][j].set_xlabel('Expression ' + all_reps_phases[i])\n",
    "                ax_e[i][j].set_ylabel('Expression ' + all_reps_phases[j])\n",
    "\n",
    "                # Correlations\n",
    "                r, p = scipy.stats.pearsonr(all_expressions[i], all_expressions[j])\n",
    "                rho, prho = scipy.stats.spearmanr(all_expressions[i], all_expressions[j])\n",
    "                tau, ptau = scipy.stats.kendalltau(all_expressions[i], all_expressions[j])\n",
    "                ax_e[i][j].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (-1, -0.7))\n",
    "\n",
    "                #ax_e[i][j].set_xlim(-10, 10)\n",
    "                #ax_e[i][j].set_ylim(-1.05, 1.05)\n",
    "\n",
    "                #ax_e[i][j].set_xticks(np.arange(-0.5, 20, step=1))\n",
    "                #ax_e[i][j].set_xticklabels(np.arange(0, 20, step=1))\n",
    "\n",
    "                #ax_e[i][j].set_yticks(np.arange(-1, 1.1, step=0.1))\n",
    "                #ax_e[i][j].set_yticklabels(np.arange(-1, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "            else:\n",
    "                raise TypeError('FIX IT!')\n",
    "\n",
    "\n",
    "    fig_e.show()\n",
    "    fig_e.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_expr_vs_expr.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "    #================================================================================================================    \n",
    "\n",
    "    # Draw express diff scatterplots\n",
    "\n",
    "    fig_s, ax_s = plt.subplots(rep_number,rep_number)\n",
    "    fig_s.set_size_inches(7*rep_number,7*rep_number)\n",
    "    fig_s.suptitle('Terminator '+eff_measure+' diff  vs. expression diff: '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "    for i in range(rep_number): # rows\n",
    "        for j in range(rep_number): # columns\n",
    "\n",
    "            if is_expr_norm:\n",
    "\n",
    "                ax_s[i][j].set_title(all_reps_phases[i]+' - '+all_reps_phases[j] , fontsize=10)\n",
    "\n",
    "                ax_s[i][j].scatter(expr_diffs[i][j], eff_diffs[i][j], s=np.array(log_phase_exprs[i][j])*scale_factor, c=np.log10(log_phase_effs[i][j]))\n",
    "                #ax_s[1].scatter(expr_diffs, eff_diffs, s=np.array(log_phase_exprs)*scale_factor, c=np.log10(log_phase_effs))\n",
    "                if j == (rep_number - 1):\n",
    "                    cax = ax_s[i][j].inset_axes([1.05, 0.1, 0.05, 0.9], transform=ax_s[i][j].transAxes)\n",
    "                    colorbar = fig_s.colorbar(pc, cax=cax)\n",
    "                    colorbar.set_label(\"log10(initial \"+eff_measure+\")\", rotation=270, va=\"baseline\")           \n",
    "                if i != j:\n",
    "                    ## Obtain m (slope) and b(intercept) of linear regression line\n",
    "                    m, b = np.polyfit(np.array(expr_diffs[i][j]), np.array(eff_diffs[i][j]), 1)\n",
    "\n",
    "                    ## Add linear regression line to scatterplot \n",
    "                    ax_s[i][j].plot(np.array(expr_diffs[i][j]), m*np.array(expr_diffs[i][j])+b)\n",
    "\n",
    "                ax_s[i][j].set_xlabel('Expression diff')\n",
    "                ax_s[i][j].set_ylabel('Terminator '+eff_measure+' diff')\n",
    "\n",
    "                # Correlations\n",
    "                r, p = scipy.stats.pearsonr(expr_diffs[i][j], eff_diffs[i][j])\n",
    "                rho, prho = scipy.stats.spearmanr(expr_diffs[i][j], eff_diffs[i][j])\n",
    "                tau, ptau = scipy.stats.kendalltau(expr_diffs[i][j], eff_diffs[i][j])\n",
    "                ax_s[i][j].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (-1, -0.7))\n",
    "\n",
    "                ax_s[i][j].set_xlim(-10, 10)\n",
    "                ax_s[i][j].set_ylim(-1.05, 1.05)\n",
    "\n",
    "\n",
    "                ax_s[i][j].set_xticks(np.arange(-10, 10, step=1))\n",
    "                ax_s[i][j].set_xticklabels(np.arange(-10, 10, step=1))\n",
    "\n",
    "                ax_s[i][j].set_yticks(np.arange(-1, 1.1, step=0.1))\n",
    "                ax_s[i][j].set_yticklabels(np.arange(-1, 1.1, step=0.1).round(decimals=1))\n",
    "\n",
    "            \n",
    "    fig_s.show()\n",
    "    \n",
    "    if is_counts:\n",
    "        fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_cnts_all_diff.pdf', bbox_inches='tight')\n",
    "    else:\n",
    "        fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_rates_all_diff.pdf', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural features vs effeciencies (rep averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_colnames = []\n",
    "eff_colnames = []\n",
    "str_colnames = ['dG','Number_of_A_around_5_prime','Number_of_U_around_5_prime','stem_length_left(nt)','Loop_length(nt)','stem_length_right(nt)','GC_content_of_stem_region(left)']\n",
    "\n",
    "\n",
    "for phase in phases:\n",
    "    expr_colnames.append(phase+'_avg_norm_expr')\n",
    "    \n",
    "    if eff_measure == 'efficiency':\n",
    "        eff_colnames.append(phase+'_avg_eff')\n",
    "    elif eff_measure == 'readthrough':\n",
    "        eff_colnames.append(phase+'_rt_avg')\n",
    "    elif eff_measure == 'reinitiation':\n",
    "        eff_colnames.append(phase+'_ri_avg')\n",
    "    elif eff_measure == 'termination':\n",
    "        eff_colnames.append(phase+'_t_avg')\n",
    "    else: \n",
    "        raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "                      \n",
    "fig_s, ax_s = plt.subplots(len(phases),len(str_colnames))\n",
    "fig_s.set_size_inches(7*len(str_colnames),7*len(phases))\n",
    "\n",
    "fig_s.suptitle('Terminator '+eff_measure+' vs. structural features: '+ experiment_set+', replicate averages')\n",
    "\n",
    "for i in range(len(phases)):\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    for j in range(len(str_colnames)):\n",
    "        str_colname = str_colnames[j]\n",
    "    \n",
    "        eff_colname = eff_colnames[i]\n",
    "        # Drop NA values (and ones?)\n",
    "        curr_final_terms_df = final_terms_df[[expr_colname, str_colname, eff_colname]].dropna(how='any')\n",
    "\n",
    "        # Delelte eff = 1?\n",
    "        if is_eff_1_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "\n",
    "        # Delelte eff = 0?\n",
    "        if is_eff_0_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "\n",
    "        # Delete negative efficiencies?\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "\n",
    "        # Delete efficiencises > max_eff_th\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "\n",
    "\n",
    "        # Delete expressions < treshold\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "\n",
    "        #expressions = curr_final_terms_df[expr_colname].tolist()\n",
    "\n",
    "\n",
    "        str_features = curr_final_terms_df[str_colname].tolist()\n",
    "        efficiencies = curr_final_terms_df[eff_colname].tolist()\n",
    "\n",
    "        ax_s[i,j].set_title(eff_colname+' '+str_colname, fontsize=10)\n",
    "        \n",
    "        if j == 0 or j == len(str_colnames) - 1: # dG or GC content\n",
    "            # Scatterplot\n",
    "            ax_s[i,j].scatter(str_features, efficiencies, s=4)\n",
    "\n",
    "            ax_s[i,j].set_xlabel(str_colname)\n",
    "            ax_s[i,j].set_ylabel('Terminator '+eff_measure + ' rate')\n",
    "\n",
    "            # Correlations\n",
    "\n",
    "            r, p = scipy.stats.pearsonr(str_features, efficiencies)\n",
    "            rho, prho = scipy.stats.spearmanr(str_features, efficiencies)\n",
    "            tau, ptau = scipy.stats.kendalltau(str_features, efficiencies)\n",
    "            #ax_s[i,j].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (0.2, 280), xycoords='axes points')\n",
    "\n",
    "            #ax_s[i,j].set_xlim(-0.5, max_expr_th+0.5)\n",
    "            ax_s[i,j].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "            #ax_s[i,j].set_xticks(np.arange(0, max_expr_th, step=0.5))\n",
    "            #ax_s[i,j].set_xticklabels(np.arange(0, max_expr_th, step=0.5))\n",
    "\n",
    "            ax_s[i,j].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "            ax_s[i,j].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            # Box plots\n",
    "            ## Prepare data for box plots\n",
    "            boxplot_data = []\n",
    "            #print(str_features)\n",
    "            unique_str_ft_values = sorted(list(set(str_features)))\n",
    "            #print(unique_str_ft_values)\n",
    "            xlabels = []\n",
    "            for ft_value in unique_str_ft_values:\n",
    "                indeces = [idx for idx in range(len(str_features)) if str_features[idx] == ft_value]\n",
    "                #print('indexes:')\n",
    "                #print(indeces)\n",
    "                effcs = [efficiencies[q] for q in indeces]\n",
    "                xlabels.append(ft_value)\n",
    "                boxplot_data.append(effcs)\n",
    "\n",
    "            # Creating plot\n",
    "            ax_s[i,j].set_xlabel(str_colname)\n",
    "            ax_s[i,j].set_ylabel('Terminator '+eff_measure + ' rate')\n",
    "            bp = ax_s[i,j].boxplot(boxplot_data,labels=xlabels,bootstrap=1000)\n",
    "            \n",
    "        # Correlations\n",
    "\n",
    "        r, p = scipy.stats.pearsonr(str_features, efficiencies)\n",
    "        rho, prho = scipy.stats.spearmanr(str_features, efficiencies)\n",
    "        tau, ptau = scipy.stats.kendalltau(str_features, efficiencies)\n",
    "        ax_s[i,j].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (0.2, 280), xycoords='axes points')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "fig_s.show()\n",
    "fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_STR_phases.pdf', bbox_inches='tight')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural features vs effeciencies (each rep and phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "str_colnames = ['dG','Number_of_A_around_5_prime','Number_of_U_around_5_prime','stem_length_left(nt)','Loop_length(nt)','stem_length_right(nt)','GC_content_of_stem_region(left)']\n",
    "eff_colnames = []\n",
    "\n",
    "for phase in phases:\n",
    "    if phase == 'log':\n",
    "        for rep in log_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')        \n",
    "                            \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('log_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('log_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                \n",
    "    elif phase == 'stat':\n",
    "        for rep in stat_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "                \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('stat_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "        \n",
    "fig_s, ax_s = plt.subplots(rep_number,len(str_colnames))\n",
    "fig_s.set_size_inches(7*len(str_colnames),7*rep_number)\n",
    "fig_s.suptitle('Terminator '+eff_measure+' vs. structural features: '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "for i in range(rep_number):\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    for j in range(len(str_colnames)):\n",
    "        str_colname = str_colnames[j]\n",
    "    \n",
    "        eff_colname = eff_colnames[i]\n",
    "        # Drop NA values (and ones?)\n",
    "        curr_final_terms_df = final_terms_df[[expr_colname, str_colname, eff_colname]].dropna(how='any')\n",
    "\n",
    "        # Delelte eff = 1?\n",
    "        if is_eff_1_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "\n",
    "        # Delelte eff = 0?\n",
    "        if is_eff_0_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "\n",
    "        # Delete negative efficiencies?\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "\n",
    "        # Delete efficiencises > max_eff_th\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "\n",
    "\n",
    "        # Delete expressions < treshold\n",
    "        if is_neg_eff_deleted:\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "            curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "\n",
    "        #expressions = curr_final_terms_df[expr_colname].tolist()\n",
    "\n",
    "\n",
    "        str_features = curr_final_terms_df[str_colname].tolist()\n",
    "        efficiencies = curr_final_terms_df[eff_colname].tolist()\n",
    "\n",
    "        ax_s[i,j].set_title(eff_colname+' '+str_colname, fontsize=10)\n",
    "        \n",
    "        if j == 0 or j == len(str_colnames) - 1: # dG or GC content\n",
    "            # Scatterplot\n",
    "            ax_s[i,j].scatter(str_features, efficiencies, s=4)\n",
    "\n",
    "            ax_s[i,j].set_xlabel(str_colname)\n",
    "            ax_s[i,j].set_ylabel('Terminator '+eff_measure + ' rate')\n",
    "\n",
    "            # Correlations\n",
    "\n",
    "            r, p = scipy.stats.pearsonr(str_features, efficiencies)\n",
    "            rho, prho = scipy.stats.spearmanr(str_features, efficiencies)\n",
    "            tau, ptau = scipy.stats.kendalltau(str_features, efficiencies)\n",
    "            #ax_s[i,j].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (0.2, 250), xycoords='axes points')\n",
    "\n",
    "            #ax_s[i,j].set_xlim(-0.5, max_expr_th+0.5)\n",
    "            ax_s[i,j].set_ylim(-0.05, 1.05)\n",
    "\n",
    "\n",
    "            #ax_s[i,j].set_xticks(np.arange(0, max_expr_th, step=0.5))\n",
    "            #ax_s[i,j].set_xticklabels(np.arange(0, max_expr_th, step=0.5))\n",
    "\n",
    "            ax_s[i,j].set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "            ax_s[i,j].set_yticklabels(np.arange(0, 1.1, step=0.1).round(decimals=1))\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            # Box plots\n",
    "            ## Prepare data for box plots\n",
    "            boxplot_data = []\n",
    "            unique_str_ft_values = sorted(list(set(str_features)))\n",
    "            xlabels = []\n",
    "            for ft_value in unique_str_ft_values:\n",
    "                indeces = [idx for idx in range(len(str_features)) if str_features[idx] == ft_value]\n",
    "                effcs = [efficiencies[q] for q in indeces]\n",
    "                xlabels.append(ft_value)\n",
    "                boxplot_data.append(effcs)\n",
    "\n",
    "            # Creating plot\n",
    "            ax_s[i,j].set_xlabel(str_colname)\n",
    "            ax_s[i,j].set_ylabel('Terminator '+eff_measure + ' rate')\n",
    "            bp = ax_s[i,j].boxplot(boxplot_data,labels=xlabels,bootstrap=1000)\n",
    "            \n",
    "        # Correlations\n",
    "\n",
    "        r, p = scipy.stats.pearsonr(str_features, efficiencies)\n",
    "        rho, prho = scipy.stats.spearmanr(str_features, efficiencies)\n",
    "        tau, ptau = scipy.stats.kendalltau(str_features, efficiencies)\n",
    "        ax_s[i,j].annotate('    Person R = '+ str(round(r,1)) + ', p = '+ str(round(p,2))+ '\\n    Spearman rho =  '+ str(round(rho,1))+ ', p = '+ str(round(prho,4))+ '\\n    Kendall tau =  '+ str(round(tau,1))+ ', p = '+ str(round(ptau,4)), (0.2, 280), xycoords='axes points')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "fig_s.show()\n",
    "fig_s.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_STR_reps_phases.pdf', bbox_inches='tight')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export fasta file with all terminators for blast-based overlap with experimental ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All terminators before filtration (from terms_df)\n",
    "fasta_writer = open(all_Ju_terms_fasta_file_path, 'w')\n",
    "for index, row in terms_df.iterrows():\n",
    "    fasta_writer.write('> ' + str(row['TTS_position'])+'_'+row['TTS_strand'] + '\\n')\n",
    "    fasta_writer.write(row[\"TTS_sequence(include_8nt_of_each_flank_sequence)\"].replace('U','T') + '\\n')\n",
    "fasta_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termination vs redthrough vs reinitiation for high and low expressed genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high_expr_th = 1\n",
    "\n",
    "rep_cnt = 0\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Average proportion of each termination mode depends on gene expression level\", fontsize=16)\n",
    "for phase in phases:\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "        \n",
    "    for rep in reps:\n",
    "        rep_cnt += 1\n",
    "\n",
    "        # Import data\n",
    "        term_mode_df = final_terms_df[['TTS_id', phase+'_'+rep+'_pc_t', phase+'_'+rep+'_pc_rt', phase+'_'+rep+'_pc_ri', phase+'_'+rep+'_norm_expr']]\n",
    "\n",
    "        # Drop terminators with zero trancripts around\n",
    "        term_mode_df = term_mode_df.dropna(how='any')\n",
    "        \n",
    "        term_mode_h_expr = term_mode_df[term_mode_df[phase+'_'+rep+'_norm_expr'] >= low_high_expr_th]\n",
    "        term_mode_l_expr = term_mode_df[term_mode_df[phase+'_'+rep+'_norm_expr'] < low_high_expr_th]\n",
    "        \n",
    "        t_h = term_mode_h_expr[phase+'_'+rep+'_pc_t'].tolist()\n",
    "        rt_h = term_mode_h_expr[phase+'_'+rep+'_pc_rt'].tolist()\n",
    "        ri_h = term_mode_h_expr[phase+'_'+rep+'_pc_ri'].tolist()\n",
    "        \n",
    "        t_l = term_mode_l_expr[phase+'_'+rep+'_pc_t'].tolist()\n",
    "        rt_l = term_mode_l_expr[phase+'_'+rep+'_pc_rt'].tolist()\n",
    "        ri_l = term_mode_l_expr[phase+'_'+rep+'_pc_ri'].tolist()\n",
    "        \n",
    "        #print(t_l)\n",
    "        \n",
    "        t_h_avg =  sum(t_h) / len(t_h)\n",
    "        rt_h_avg =  sum(rt_h) / len(rt_h)\n",
    "        ri_h_avg =  sum(ri_h) / len(ri_h)\n",
    "        \n",
    "        t_l_avg =  sum(t_l) / len(t_l)\n",
    "        rt_l_avg =  sum(rt_l) / len(rt_l)\n",
    "        ri_l_avg =  sum(ri_l) / len(ri_l)\n",
    "\n",
    "        species = (\n",
    "            \"Low expression\",\n",
    "            \"High expression\"\n",
    "        )\n",
    "        weight_counts = {\n",
    "            \"Terminated\": np.array([t_l_avg, t_h_avg]),\n",
    "            \"Read-through\": np.array([rt_l_avg, rt_h_avg]),\n",
    "            \"Reinitiated\": np.array([ri_l_avg, ri_h_avg])\n",
    "        }\n",
    "        width = 0.5\n",
    "\n",
    "        ax = fig.add_subplot(2, 5, rep_cnt)\n",
    "        \n",
    "        bottom = np.zeros(2)\n",
    "        for boolean, weight_count in weight_counts.items():\n",
    "            p = ax.bar(species, weight_count, width, label=boolean, bottom=bottom)\n",
    "            bottom += weight_count\n",
    "\n",
    "        ax.set_title(phase+'_'+rep)\n",
    "        ax.legend(loc=\"lower center\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_mode_proprtion.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readthrough efficiency and reinitiation eff for high and low expressed genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cnt = 0\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "fig.suptitle('Readthrough rate distribution depends on expression level', fontsize=16)\n",
    "for phase in phases:\n",
    "    \n",
    "    if phase == 'log':\n",
    "        reps = log_reps\n",
    "    elif phase == 'stat':\n",
    "        reps = stat_reps\n",
    "        \n",
    "    for rep in reps:\n",
    "        rep_cnt += 1\n",
    "\n",
    "        # Import data\n",
    "        term_mode_df = final_terms_df[['TTS_id', phase+'_'+rep+'_pc_t', phase+'_'+rep+'_pc_rt', phase+'_'+rep+'_pc_ri', phase+'_'+rep+'_norm_expr']]\n",
    "\n",
    "        # Drop terminators with zero trancripts around\n",
    "        term_mode_df = term_mode_df.dropna(how='any')\n",
    "        \n",
    "        term_mode_h_expr = term_mode_df[term_mode_df[phase+'_'+rep+'_norm_expr'] >= low_high_expr_th]\n",
    "        term_mode_l_expr = term_mode_df[term_mode_df[phase+'_'+rep+'_norm_expr'] < low_high_expr_th]\n",
    "        \n",
    "        t_h = term_mode_h_expr[phase+'_'+rep+'_pc_t'].tolist()\n",
    "        rt_h = term_mode_h_expr[phase+'_'+rep+'_pc_rt'].tolist()\n",
    "        ri_h = term_mode_h_expr[phase+'_'+rep+'_pc_ri'].tolist()\n",
    "        \n",
    "        before_tr_h_with0 = [m+n for m, n in zip(t_h, rt_h)] # all trancripts in 'before' window\n",
    "        \n",
    "        rt_eff_h = []\n",
    "        ri_eff_h = []\n",
    "        for i in range(len(before_tr_h_with0)):\n",
    "            before = before_tr_h_with0[i]\n",
    "            if before == 0:\n",
    "                continue\n",
    "            else:\n",
    "                rt_eff_h.append(rt_h[i]/before)\n",
    "                if ri_h[i]/before <= 1:\n",
    "                    ri_eff_h.append(ri_h[i]/before)\n",
    "                \n",
    "        t_l = term_mode_l_expr[phase+'_'+rep+'_pc_t'].tolist()\n",
    "        rt_l = term_mode_l_expr[phase+'_'+rep+'_pc_rt'].tolist()\n",
    "        ri_l = term_mode_l_expr[phase+'_'+rep+'_pc_ri'].tolist()\n",
    "        \n",
    "        before_tr_l_with0 = [m+n for m, n in zip(t_l, rt_l)] # all trancripts in 'before' window\n",
    "            \n",
    "        rt_eff_l = []\n",
    "        ri_eff_l = []       \n",
    "        for i in range(len(before_tr_l_with0)):\n",
    "            before = before_tr_l_with0[i]\n",
    "            if before == 0:\n",
    "                continue\n",
    "            else:\n",
    "                rt_eff_l.append(rt_l[i]/before)\n",
    "                if ri_l[i]/before <= 1:\n",
    "                    ri_eff_l.append(ri_l[i]/before)        \n",
    "        \n",
    "\n",
    "        ax = fig.add_subplot(2, 5, rep_cnt)\n",
    "        \n",
    "        ax.set_title('Default violin plot')\n",
    "        ax.set_ylabel('Readthrough rate')\n",
    "        ax.set_xlabel('Expression level')\n",
    "        ax.violinplot([rt_eff_l, rt_eff_h], showmeans=True)\n",
    "        ax.set_xticklabels(['','low', '', 'high'])\n",
    "        #ax.set_xticklabels(['low', 'high'])\n",
    "        ax.set_title(phase+'_'+rep)\n",
    "        ax.legend(loc=\"lower center\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_mode_violin.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "expr_colnames = []\n",
    "eff_colnames = []\n",
    "\n",
    "for phase in phases:\n",
    "    if phase == 'log':\n",
    "        for rep in log_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')       \n",
    "                            \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('log_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('log_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                \n",
    "    elif phase == 'stat':\n",
    "        for rep in stat_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "                \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('stat_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "   \n",
    "            \n",
    "           \n",
    "#fig_s, ax_s = plt.subplots(rep_number,1)\n",
    "#fig_s.set_size_inches(7*1,7*rep_number)\n",
    "#fig_s.suptitle('Terminator '+eff_measure+' vs. expression: '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "for i in range(rep_number):\n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df.dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "    #print(curr_final_terms_df.columns[:30])\n",
    "\n",
    "        \n",
    "    #ax_s[i].set_title(eff_colname, fontsize=10)\n",
    "    \n",
    "    #++++++++++++\n",
    "    \n",
    "\n",
    "\n",
    "    df = curr_final_terms_df\n",
    "    #str_features = ['dG', 'Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','stem_length_left(nt)', 'Loop_length(nt)', 'GC_content_of_stem_region(left)']\n",
    "    str_features = ['Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','Loop_length(nt)']\n",
    "    features =  [expr_colname] + str_features\n",
    "    X = df[features]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(X_std)\n",
    "\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "    fig = px.scatter(components, x=0, y=1, color=np.log10(df[eff_colname]))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        fig.add_annotation(\n",
    "            ax=0, ay=0,\n",
    "            axref=\"x\", ayref=\"y\",\n",
    "            x=loadings[i, 0],\n",
    "            y=loadings[i, 1],\n",
    "            showarrow=True,\n",
    "            arrowsize=2,\n",
    "            arrowhead=2,\n",
    "            xanchor=\"right\",\n",
    "            yanchor=\"top\"\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            x=loadings[i, 0],\n",
    "            y=loadings[i, 1],\n",
    "            ax=0, ay=0,\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"bottom\",\n",
    "            text=feature,\n",
    "            yshift=5,\n",
    "        )\n",
    "    fig.show()\n",
    "\n",
    "#fig.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_PCA_reps_phases.pdf', bbox_inches='tight')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "expr_colnames = []\n",
    "eff_colnames = []\n",
    "\n",
    "for phase in phases:\n",
    "    if phase == 'log':\n",
    "        for rep in log_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')       \n",
    "                            \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('log_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('log_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                \n",
    "    elif phase == 'stat':\n",
    "        for rep in stat_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "                \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('stat_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "   \n",
    "            \n",
    "           \n",
    "#fig_s, ax_s = plt.subplots(rep_number,1)\n",
    "#fig_s.set_size_inches(7*1,7*rep_number)\n",
    "#fig_s.suptitle('Terminator '+eff_measure+' vs. expression: '+ experiment_set+', each phase & replicate seperately')\n",
    "\n",
    "for i in range(rep_number):\n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df.dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "\n",
    "    \n",
    "    #++++++++++++\n",
    "    \n",
    "\n",
    "    str_features = ['dG', 'Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','stem_length_left(nt)', 'Loop_length(nt)', 'GC_content_of_stem_region(left)']\n",
    "    #str_features = ['Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','Loop_length(nt)']\n",
    "    features =  [expr_colname] + str_features\n",
    "    \n",
    "    # Load your dataset (replace 'data.csv' with your actual dataset)\n",
    "    data = curr_final_terms_df\n",
    "\n",
    "    # Split the dataset into features (X) and target variable (y)\n",
    "    X = data[features]\n",
    "    y = data[eff_colname]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    #y_pred = model.predict(X_test)\n",
    "\n",
    "    # Bootstrap to estimate confidence intervals for feature coefficients\n",
    "    n_bootstraps = 1000\n",
    "    bootstrap_coefs = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        #X_boot, X_test_boot, y_boot, y_test_boot = train_test_split(X, y, test_size=0.2)#, random_state=42)\n",
    "        X_boot, y_boot = resample(X_train, y_train)\n",
    "        model_b = LinearRegression()\n",
    "        model_b.fit(X_boot, y_boot)\n",
    "        bootstrap_coefs.append(model_b.coef_)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    confidence_intervals = np.percentile(bootstrap_coefs, [25, 75], axis=0)\n",
    "    #print(confidence_intervals)\n",
    "    \n",
    "    # Calculate medians as central value estimators\n",
    "    #print(bootstrap_importances)\n",
    "    median_importances = np.median(bootstrap_coefs, axis=0)\n",
    "    #print(median_importances)\n",
    "    #print(model.feature_importances_)\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': median_importances, 'ci_l': confidence_intervals[0], 'ci_h': confidence_intervals[1]})\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Visualize feature importance with confidence intervals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')\n",
    "    plt.errorbar(x=feature_importance['Importance'], y=feature_importance['Feature'],\n",
    "                 xerr=[feature_importance['Importance'] - feature_importance['ci_l'], feature_importance['ci_h'] - feature_importance['Importance']],\n",
    "                 fmt='o', color='black', capsize=5, capthick=2)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Coefficients with IQRs')\n",
    "    plt.show()\n",
    "\n",
    "    # Display feature importance table with confidence intervals\n",
    "    print('\\nFeature Coefficients with IQRs:')\n",
    "    print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF: reps & phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "expr_colnames = []\n",
    "eff_colnames = []\n",
    "\n",
    "for phase in phases:\n",
    "    if phase == 'log':\n",
    "        for rep in log_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')        \n",
    "                            \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('log_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('log_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                \n",
    "    elif phase == 'stat':\n",
    "        for rep in stat_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "                \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('stat_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "   \n",
    "            \n",
    "           \n",
    "fig_rf, ax_rf = plt.subplots(rep_number,1)\n",
    "fig_rf.set_size_inches(10,7*rep_number)\n",
    "fig_rf.suptitle('Feature Importance (Random Forest) with IQR: each phase & replicate seperately')\n",
    "\n",
    "for i in range(rep_number):\n",
    "    \n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df.dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "    \n",
    "    #++++++++++++\n",
    "    \n",
    "\n",
    "\n",
    "    str_features = ['dG', 'Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','stem_length_left(nt)', 'Loop_length(nt)', 'GC_content_of_stem_region(left)']\n",
    "    #str_features = ['Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','Loop_length(nt)']\n",
    "    features =  [expr_colname] + str_features\n",
    "    \n",
    "    # Load your dataset (replace 'data.csv' with your actual dataset)\n",
    "    data = curr_final_terms_df\n",
    "    \n",
    "    X = data[features]\n",
    "    y = data[eff_colname]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    # Train the Random Forest regression model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    #y_pred = model.predict(X_test)\n",
    "\n",
    "    # Bootstrap to estimate confidence intervals for feature importances\n",
    "    n_bootstraps = 1000\n",
    "    bootstrap_importances = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        X_boot, y_boot = resample(X_train, y_train)\n",
    "        model.fit(X_boot, y_boot)\n",
    "        bootstrap_importances.append(model.feature_importances_)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    confidence_intervals = np.percentile(bootstrap_importances, [25, 75], axis=0)\n",
    "    #print(confidence_intervals)\n",
    "    \n",
    "    # Calculate medians as central value estimators\n",
    "    #print(bootstrap_importances)\n",
    "    median_importances = np.median(bootstrap_importances, axis=0)\n",
    "    #print(median_importances)\n",
    "    #print(model.feature_importances_)\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': median_importances, 'ci_l': confidence_intervals[0], 'ci_h': confidence_intervals[1]})\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Visualize feature importance with confidence intervals\n",
    "    ax_rf[i].set_title(all_reps_phases[i], fontsize=10)\n",
    "    ax_rf[i].barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')\n",
    "    \n",
    "    ax_rf[i].errorbar(x=feature_importance['Importance'], y=feature_importance['Feature'],\n",
    "                 xerr=[feature_importance['Importance'] - feature_importance['ci_l'], feature_importance['ci_h'] - feature_importance['Importance']],\n",
    "                 fmt='o', color='black', capsize=5, capthick=2)\n",
    "    ax_rf[i].set_xlabel('Importance')\n",
    "    ax_rf[i].set_ylabel('Feature')\n",
    "\n",
    "fig_rf.show()\n",
    "fig_rf.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_RF_reps_phases.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF: phases (rep avereges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_colnames = []\n",
    "expr_colnames = []\n",
    "for phase in phases:\n",
    "    expr_colnames.append(phase+'_avg_norm_expr')\n",
    "\n",
    "    if eff_measure == 'efficiency':\n",
    "        eff_colnames.append(phase+'_avg_eff')\n",
    "    elif eff_measure == 'readthrough':\n",
    "        eff_colnames.append(phase+'_rt_avg')\n",
    "    elif eff_measure == 'reinitiation':\n",
    "        eff_colnames.append(phase+'_ri_avg')\n",
    "    elif eff_measure == 'termination':\n",
    "        eff_colnames.append(phase+'_t_avg')\n",
    "    else: \n",
    "        raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "            \n",
    "\n",
    "fig_rf, ax_rf = plt.subplots(len(phases),1)\n",
    "fig_rf.set_size_inches(10,7*len(phases))\n",
    "fig_rf.suptitle('Feature Importance (Random Forest) with IQR: phase averages')\n",
    "            \n",
    "for i in range(len(phases)):\n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df.dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "\n",
    "        \n",
    "    #ax_s[i].set_title(eff_colname, fontsize=10)\n",
    "    \n",
    "    #++++++++++++\n",
    "    \n",
    "\n",
    "\n",
    "    str_features = ['dG', 'Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','stem_length_left(nt)', 'Loop_length(nt)', 'GC_content_of_stem_region(left)']\n",
    "    #str_features = ['Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','Loop_length(nt)']\n",
    "    features =  [expr_colname] + str_features\n",
    "    #print(expr_colname)\n",
    "    # Load your dataset (replace 'data.csv' with your actual dataset)\n",
    "    data = curr_final_terms_df[features + [eff_colname]]\n",
    "    \n",
    "    X = data[features]\n",
    "    y = data[eff_colname]\n",
    "    #display(data.head())\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    # Train the Random Forest regression model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    #y_pred = model.predict(X_test)\n",
    "\n",
    "    # Bootstrap to estimate confidence intervals for feature importances\n",
    "    n_bootstraps = 1000\n",
    "    bootstrap_importances = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        X_boot, y_boot = resample(X_train, y_train)\n",
    "        model.fit(X_boot, y_boot)\n",
    "        bootstrap_importances.append(model.feature_importances_)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    confidence_intervals = np.percentile(bootstrap_importances, [25, 75], axis=0)\n",
    "    \n",
    "    # Calculate medians as central value estimators\n",
    "    median_importances = np.median(bootstrap_importances, axis=0)\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': median_importances, 'ci_l': confidence_intervals[0], 'ci_h': confidence_intervals[1]})\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Visualize feature importance with confidence intervals\n",
    "    ax_rf[i].set_title(phases[i], fontsize=10)\n",
    "    ax_rf[i].barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')\n",
    "    \n",
    "    ax_rf[i].errorbar(x=feature_importance['Importance'], y=feature_importance['Feature'],\n",
    "                 xerr=[feature_importance['Importance'] - feature_importance['ci_l'], feature_importance['ci_h'] - feature_importance['Importance']],\n",
    "                 fmt='o', color='black', capsize=5, capthick=2)\n",
    "    ax_rf[i].set_xlabel('Importance')\n",
    "    ax_rf[i].set_ylabel('Feature')\n",
    "\n",
    "fig_rf.show()\n",
    "fig_rf.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_RF_phases.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PC: reps & phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pingouin as pg\n",
    "\n",
    "rep_number = len(log_reps) + len(stat_reps)\n",
    "all_reps_phases = log_reps + stat_reps\n",
    "\n",
    "expr_colnames = []\n",
    "eff_colnames = []\n",
    "\n",
    "for phase in phases:\n",
    "    if phase == 'log':\n",
    "        for rep in log_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('log_'+rep+'_norm_expr')       \n",
    "                            \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('log_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('log_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('log_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('log_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "                \n",
    "    elif phase == 'stat':\n",
    "        for rep in stat_reps:\n",
    "            if is_expr_norm:\n",
    "                expr_colnames.append('stat_'+rep+'_norm_expr')\n",
    "                \n",
    "            if eff_measure == 'efficiency':\n",
    "                eff_colnames.append('stat_'+rep+'_eff')\n",
    "            elif eff_measure == 'readthrough':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_rt')\n",
    "            elif eff_measure == 'reinitiation':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_ri')\n",
    "            elif eff_measure == 'termination':\n",
    "                eff_colnames.append('stat_'+rep+'_pc_t')\n",
    "            else: \n",
    "                raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "   \n",
    "\n",
    "fig_pc, ax_pc = plt.subplots(rep_number,1)\n",
    "fig_pc.set_size_inches(10,6*rep_number)\n",
    "fig_pc.suptitle('Partial correlation of each terminator feature and readthrough rate (with IQRs): each phase & replicate seperately')\n",
    "\n",
    "for i in range(rep_number):\n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df.dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "    #print(curr_final_terms_df.columns[:30])\n",
    "\n",
    "        \n",
    "    #ax_s[i].set_title(eff_colname, fontsize=10)\n",
    "    \n",
    "    #++++++++++++\n",
    "\n",
    "    str_features = ['dG', 'Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','stem_length_left(nt)', 'Loop_length(nt)', 'GC_content_of_stem_region(left)']\n",
    "    #str_features = ['Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','Loop_length(nt)']\n",
    "    features =  [expr_colname] + str_features\n",
    "    \n",
    "    # Load your dataset (replace 'data.csv' with your actual dataset)\n",
    "    data = curr_final_terms_df[features + [eff_colname]]\n",
    "\n",
    "    # Compute partial correlation coefficients between each predictor variable and the target variable\n",
    "    partial_correlations = {}\n",
    "    for column in data.columns:\n",
    "        if column in features:\n",
    "            # Bootstrap to estimate confidence intervals for partial correlation coefficients\n",
    "            partial_corrs_bootstrap = []\n",
    "            for _ in range(1000):\n",
    "                boot_sample = resample(data, replace=True)\n",
    "                partial_corr = pg.partial_corr(data=boot_sample, x=column, y=eff_colname, covar=data.drop(columns=[column, eff_colname]).columns.tolist())\n",
    "                partial_corrs_bootstrap.append(partial_corr['r'].values[0])\n",
    "                #print(partial_corrs_bootstrap)\n",
    "            # Calculate confidence intervals\n",
    "            ci = np.percentile(partial_corrs_bootstrap, [25, 75])\n",
    "            partial_correlations[column] = {'Partial Correlation': pg.partial_corr(data=data, x=column, y=eff_colname, covar=data.drop(columns=[column, eff_colname]).columns.tolist())['r'].values[0], 'CI Lower': ci[0], 'CI Upper': ci[1]}\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    partial_correlations_df = pd.DataFrame(partial_correlations).T.reset_index().rename(columns={'index': 'Feature'})\n",
    "\n",
    "    # Sort features by absolute partial correlation coefficient\n",
    "    partial_correlations_df['Absolute Partial Correlation'] = np.abs(partial_correlations_df['Partial Correlation'])\n",
    "    partial_correlations_df = partial_correlations_df.sort_values(by='Absolute Partial Correlation', ascending=False).drop(columns='Absolute Partial Correlation')\n",
    "\n",
    "    # Visualize partial correlation coefficients with confidence intervals\n",
    "    ax_pc[i].barh(partial_correlations_df['Feature'], partial_correlations_df['Partial Correlation'], color='skyblue')\n",
    "    ax_pc[i].errorbar(x=partial_correlations_df['Partial Correlation'], y=partial_correlations_df['Feature'],\n",
    "                 xerr=[partial_correlations_df['Partial Correlation'] - partial_correlations_df['CI Lower'], partial_correlations_df['CI Upper'] - partial_correlations_df['Partial Correlation']],\n",
    "                 fmt='o', color='black', capsize=5, capthick=2)\n",
    "    ax_pc[i].set_xlabel('Partial Correlation')\n",
    "    ax_pc[i].set_ylabel('Feature')\n",
    "\n",
    "fig_pc.show()\n",
    "fig_pc.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_PC_reps_phases.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PC: phases (rep avereges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_colnames = []\n",
    "expr_colnames = []\n",
    "for phase in phases:\n",
    "    expr_colnames.append(phase+'_avg_norm_expr')\n",
    "\n",
    "    if eff_measure == 'efficiency':\n",
    "        eff_colnames.append(phase+'_avg_eff')\n",
    "    elif eff_measure == 'readthrough':\n",
    "        eff_colnames.append(phase+'_rt_avg')\n",
    "    elif eff_measure == 'reinitiation':\n",
    "        eff_colnames.append(phase+'_ri_avg')\n",
    "    elif eff_measure == 'termination':\n",
    "        eff_colnames.append(phase+'_t_avg')\n",
    "    else: \n",
    "        raise TypeError('Unknown eff measure: '+str(eff_measure))\n",
    "\n",
    "fig_pc, ax_pc = plt.subplots(len(phases),1)\n",
    "fig_pc.set_size_inches(10,6*len(phases))\n",
    "fig_pc.suptitle('Partial correlation of each terminator feature and readthrough rate (with IQRs): phase averages')\n",
    "\n",
    "\n",
    "for i in range(len(phases)):\n",
    "\n",
    "    expr_colname = expr_colnames[i]\n",
    "    eff_colname = eff_colnames[i]\n",
    "    # Drop NA values (and ones?)\n",
    "    curr_final_terms_df = final_terms_df.dropna(how='any', subset=[expr_colname, eff_colname])\n",
    "    \n",
    "    # Delelte eff = 1?\n",
    "    if is_eff_1_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=1]\n",
    "        \n",
    "    # Delelte eff = 0?\n",
    "    if is_eff_0_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname]!=0]\n",
    "    \n",
    "    # Delete negative efficiencies?\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] >= 0]\n",
    "        \n",
    "    # Delete efficiencises > max_eff_th\n",
    "    curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[eff_colname] <= max_eff_th]\n",
    "    \n",
    "        \n",
    "    # Delete expressions < treshold\n",
    "    if is_neg_eff_deleted:\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] >= min_expr_th]\n",
    "        curr_final_terms_df = curr_final_terms_df[curr_final_terms_df[expr_colname] <= max_expr_th]\n",
    "        \n",
    "    #print(curr_final_terms_df.columns[:30])\n",
    "\n",
    "        \n",
    "    #ax_s[i].set_title(eff_colname, fontsize=10)\n",
    "    \n",
    "    #++++++++++++\n",
    "\n",
    "    str_features = ['dG', 'Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','stem_length_left(nt)', 'Loop_length(nt)', 'GC_content_of_stem_region(left)']\n",
    "    #str_features = ['Number_of_A_around_5_prime', 'Number_of_U_around_5_prime','Loop_length(nt)']\n",
    "    features =  [expr_colname] + str_features\n",
    "    \n",
    "    # Load your dataset (replace 'data.csv' with your actual dataset)\n",
    "    data = curr_final_terms_df[features + [eff_colname]]\n",
    "    #display(data.head())\n",
    "\n",
    "    # Compute partial correlation coefficients between each predictor variable and the target variable\n",
    "    partial_correlations = {}\n",
    "    for column in data.columns:\n",
    "        if column in features:\n",
    "            # Bootstrap to estimate confidence intervals for partial correlation coefficients\n",
    "            partial_corrs_bootstrap = []\n",
    "            for _ in range(1000):\n",
    "                boot_sample = resample(data, replace=True)\n",
    "                partial_corr = pg.partial_corr(data=boot_sample, x=column, y=eff_colname, covar=data.drop(columns=[column, eff_colname]).columns.tolist())\n",
    "                partial_corrs_bootstrap.append(partial_corr['r'].values[0])\n",
    "                #print(partial_corrs_bootstrap)\n",
    "            # Calculate confidence intervals\n",
    "            ci = np.percentile(partial_corrs_bootstrap, [25, 75])\n",
    "            partial_correlations[column] = {'Partial Correlation': pg.partial_corr(data=data, x=column, y=eff_colname, covar=data.drop(columns=[column, eff_colname]).columns.tolist())['r'].values[0], 'CI Lower': ci[0], 'CI Upper': ci[1]}\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    partial_correlations_df = pd.DataFrame(partial_correlations).T.reset_index().rename(columns={'index': 'Feature'})\n",
    "\n",
    "    # Sort features by absolute partial correlation coefficient\n",
    "    partial_correlations_df['Absolute Partial Correlation'] = np.abs(partial_correlations_df['Partial Correlation'])\n",
    "    partial_correlations_df = partial_correlations_df.sort_values(by='Absolute Partial Correlation', ascending=False).drop(columns='Absolute Partial Correlation')\n",
    "\n",
    "    # Visualize partial correlation coefficients with confidence intervals\n",
    "    ax_pc[i].barh(partial_correlations_df['Feature'], partial_correlations_df['Partial Correlation'], color='skyblue')\n",
    "    ax_pc[i].errorbar(x=partial_correlations_df['Partial Correlation'], y=partial_correlations_df['Feature'],\n",
    "                 xerr=[partial_correlations_df['Partial Correlation'] - partial_correlations_df['CI Lower'], partial_correlations_df['CI Upper'] - partial_correlations_df['Partial Correlation']],\n",
    "                 fmt='o', color='black', capsize=5, capthick=2)\n",
    "    ax_pc[i].set_xlabel('Partial Correlation')\n",
    "    ax_pc[i].set_ylabel('Feature')\n",
    "\n",
    "fig_pc.show()\n",
    "fig_pc.savefig(str(plots_out_folder)+'/'+term_mech_prefix+'_'+bidir_prefix+'_'+eff_measure+'_PC_phases.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TetR.ipynb",
   "provenance": [
    {
     "file_id": "1R9LJz2lFpKpZmeoaZAXfk07apPFK-hYX",
     "timestamp": 1660661132048
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
